{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# آنچه در این فایل یاد میگیرید"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  CSV /نحوه وارد کردن یک دیتاست فایل اکسل"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.    keras <- نحوه توصیف و گردآوردی یک مدل پرسپترون چند لایه "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. نحوه ارزیابی یک مدل کراس برای یک دیتاست ارزیابی (ولیدیشن دیتاست)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## مراحل فایل"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) بارگذاری دیتا"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1   2   3    4     5      6   7  8\n",
       "0  6  148  72  35    0  33.6  0.627  50  1\n",
       "1  1   85  66  29    0  26.6  0.351  31  0\n",
       "2  8  183  64   0    0  23.3  0.672  32  1\n",
       "3  1   89  66  23   94  28.1  0.167  21  0\n",
       "4  0  137  40  35  168  43.1  2.288  33  1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# وارد کردن کتابخانه\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# وارد کردن دیتاست دیابت\n",
    "dataset = pd.read_excel(\"C:\\\\Users\\\\ShahinN\\\\Desktop\\\\pima-indians-diabetes.xlsx\", header=None)\n",
    "dataset.head()                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# بخش بندی دیتاست\n",
    "X = dataset.values[:,0:8]\n",
    "Y = dataset.values[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# وارد کردن پکیج ها و ابزارهای یادگیری عمیق\n",
    "# conda install -c conda-forge keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ساخت مدل\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, kernel_initializer= 'uniform' , activation= 'relu' ))\n",
    "model.add(Dense(8, kernel_initializer= 'uniform' , activation= 'relu' ))\n",
    "model.add(Dense(1, kernel_initializer= 'uniform' , activation= 'sigmoid' ))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " تصویر زیر، ساختار شبکه  را نشان میدهد"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQEAYABgAAD/4RDyRXhpZgAATU0AKgAAAAgABAE7AAIAAAANAAAISodpAAQAAAABAAAIWJydAAEAAAAaAAAQ0OocAAcAAAgMAAAAPgAAAAAc6gAAAAgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAHNoYWhpbiBub3VyaQAAAAWQAwACAAAAFAAAEKaQBAACAAAAFAAAELqSkQACAAAAAzA4AACSkgACAAAAAzA4AADqHAAHAAAIDAAACJoAAAAAHOoAAAAIAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAyMDE5OjEwOjE4IDExOjEzOjA1ADIwMTk6MTA6MTggMTE6MTM6MDUAAABzAGgAYQBoAGkAbgAgAG4AbwB1AHIAaQAAAP/hCx9odHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvADw/eHBhY2tldCBiZWdpbj0n77u/JyBpZD0nVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkJz8+DQo8eDp4bXBtZXRhIHhtbG5zOng9ImFkb2JlOm5zOm1ldGEvIj48cmRmOlJERiB4bWxuczpyZGY9Imh0dHA6Ly93d3cudzMub3JnLzE5OTkvMDIvMjItcmRmLXN5bnRheC1ucyMiPjxyZGY6RGVzY3JpcHRpb24gcmRmOmFib3V0PSJ1dWlkOmZhZjViZGQ1LWJhM2QtMTFkYS1hZDMxLWQzM2Q3NTE4MmYxYiIgeG1sbnM6ZGM9Imh0dHA6Ly9wdXJsLm9yZy9kYy9lbGVtZW50cy8xLjEvIi8+PHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9InV1aWQ6ZmFmNWJkZDUtYmEzZC0xMWRhLWFkMzEtZDMzZDc1MTgyZjFiIiB4bWxuczp4bXA9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC8iPjx4bXA6Q3JlYXRlRGF0ZT4yMDE5LTEwLTE4VDExOjEzOjA1LjA3ODwveG1wOkNyZWF0ZURhdGU+PC9yZGY6RGVzY3JpcHRpb24+PHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9InV1aWQ6ZmFmNWJkZDUtYmEzZC0xMWRhLWFkMzEtZDMzZDc1MTgyZjFiIiB4bWxuczpkYz0iaHR0cDovL3B1cmwub3JnL2RjL2VsZW1lbnRzLzEuMS8iPjxkYzpjcmVhdG9yPjxyZGY6U2VxIHhtbG5zOnJkZj0iaHR0cDovL3d3dy53My5vcmcvMTk5OS8wMi8yMi1yZGYtc3ludGF4LW5zIyI+PHJkZjpsaT5zaGFoaW4gbm91cmk8L3JkZjpsaT48L3JkZjpTZXE+DQoJCQk8L2RjOmNyZWF0b3I+PC9yZGY6RGVzY3JpcHRpb24+PC9yZGY6UkRGPjwveDp4bXBtZXRhPg0KICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAKICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIAogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgCiAgICAgICAgICAgICAgICAgICAgICAgICAgICA8P3hwYWNrZXQgZW5kPSd3Jz8+/9sAQwAHBQUGBQQHBgUGCAcHCAoRCwoJCQoVDxAMERgVGhkYFRgXGx4nIRsdJR0XGCIuIiUoKSssKxogLzMvKjInKisq/9sAQwEHCAgKCQoUCwsUKhwYHCoqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioq/8AAEQgCGgG4AwEiAAIRAQMRAf/EAB8AAAEFAQEBAQEBAAAAAAAAAAABAgMEBQYHCAkKC//EALUQAAIBAwMCBAMFBQQEAAABfQECAwAEEQUSITFBBhNRYQcicRQygZGhCCNCscEVUtHwJDNicoIJChYXGBkaJSYnKCkqNDU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6g4SFhoeIiYqSk5SVlpeYmZqio6Slpqeoqaqys7S1tre4ubrCw8TFxsfIycrS09TV1tfY2drh4uPk5ebn6Onq8fLz9PX29/j5+v/EAB8BAAMBAQEBAQEBAQEAAAAAAAABAgMEBQYHCAkKC//EALURAAIBAgQEAwQHBQQEAAECdwABAgMRBAUhMQYSQVEHYXETIjKBCBRCkaGxwQkjM1LwFWJy0QoWJDThJfEXGBkaJicoKSo1Njc4OTpDREVGR0hJSlNUVVZXWFlaY2RlZmdoaWpzdHV2d3h5eoKDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uLj5OXm5+jp6vLz9PX29/j5+v/aAAwDAQACEQMRAD8A+kaKKKACiiigAooooAKKKKACiiigAorhNT+NXgDR9UudO1HXjDdWsrwzRiyuG2urFWGVjIOCD0NVf+F+/DX/AKGNv/Bfc/8AxugD0WivOv8Ahfvw1/6GNv8AwX3P/wAbo/4X78Nf+hjb/wAF9z/8boA9Forzr/hfvw1/6GNv/Bfc/wDxuj/hfvw1/wChjb/wX3P/AMboA9Forzr/AIX78Nf+hjb/AMF9z/8AG6P+F+/DX/oY2/8ABfc//G6APRaK86/4X78Nf+hjb/wX3P8A8bo/4X78Nf8AoY2/8F9z/wDG6APRaK86/wCF+/DX/oY2/wDBfc//ABuj/hfvw1/6GNv/AAX3P/xugD0WivOv+F+/DX/oY2/8F9z/APG6P+F+/DX/AKGNv/Bfc/8AxugD0WivOv8Ahfvw1/6GNv8AwX3P/wAbo/4X78Nf+hjb/wAF9z/8boA9Forzr/hfvw1/6GNv/Bfc/wDxuj/hfvw1/wChjb/wX3P/AMboA9Forzr/AIX78Nf+hjb/AMF9z/8AG6P+F+/DX/oY2/8ABfc//G6APRaK86/4X78Nf+hjb/wX3P8A8bo/4X78Nf8AoY2/8F9z/wDG6APRaK86/wCF+/DX/oY2/wDBfc//ABuj/hfvw1/6GNv/AAX3P/xugD0WivOv+F+/DX/oY2/8F9z/APG6P+F+/DX/AKGNv/Bfc/8AxugD0WivOv8Ahfvw1/6GNv8AwX3P/wAbo/4X78Nf+hjb/wAF9z/8boA9Forzr/hfvw1/6GNv/Bfc/wDxuj/hfvw1/wChjb/wX3P/AMboA9Forzr/AIX78Nf+hjb/AMF9z/8AG6P+F+/DX/oY2/8ABfc//G6APRaK86/4X78Nf+hjb/wX3P8A8bo/4X78Nf8AoY2/8F9z/wDG6APRaK86/wCF+/DX/oY2/wDBfc//ABuj/hfvw1/6GNv/AAX3P/xugD0WivOv+F+/DX/oY2/8F9z/APG6P+F+/DX/AKGNv/Bfc/8AxugD0WiuM8P/ABb8E+Kdbh0jQtaN1fT7vLiNnPHu2qWPLIB0UnrXZ0AFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQB418NIo5NX8eGRFYjxVe/eGf4hXefZ4f+eMf/fIrhfhj/yFvHv/AGNd7/6EK7+uqHwo5Z/EyL7PD/zxj/75FH2eH/njH/3yKloqySL7PD/zxj/75FH2eH/njH/3yKlooAi+zw/88Y/++RR9nh/54x/98ipaKAIvs8P/ADxj/wC+RR9nh/54x/8AfIqWigCL7PD/AM8Y/wDvkUfZ4f8AnjH/AN8ipaazqilnYKo6knAFADPs8P8Azxj/AO+RR9nh/wCeMf8A3yKWKeKdd0EqSD1Rgf5VJQBF9nh/54x/98ij7PD/AM8Y/wDvkVLRQBF9nh/54x/98ij7PD/zxj/75FS0UARfZ4f+eMf/AHyKPs8P/PGP/vkVLRQBF9nh/wCeMf8A3yKPs8P/ADxj/wC+RUhIUZJwPU0A56UAR/Z4f+eMf/fIo+zw/wDPGP8A75FS0UARfZ4f+eMf/fIo+zw/88Y/++RUtFAEX2eH/njH/wB8ij7PD/zxj/75FS0UARfZ4f8AnjH/AN8ij7PD/wA8Y/8AvkVLRQBF9nh/54x/98ij7PD/AM8Y/wDvkVLRQBF9nh/54x/98ij7PD/zxj/75FS0UARfZ4f+eMf/AHyKPs8P/PGP/vkVLRQBF9nh/wCeMf8A3yKPs8P/ADxj/wC+RUtFAHN6pFHH8SPA5RFXN9dZwMf8uU1ekV5zq3/JR/A//X9df+kU1ejVzVPiOin8IUUUVmaBRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQB458Mf8AkLePf+xrvf8A0IV39cB8Mf8AkLePf+xrvf8A0IV39dUPhRyz+JhRRRVkBRRRQAUUUUAFFFFABXlNrbzfFDxnqo1G6mj0HSJvs8VtA+zzpAAWLH8a9WryrwbfQeB/G2v6BrkgtItQuze2dxLwku5QCN3QEEd6mRUQ8XeDU8EaW/ibwdcXFnLYEPPbNKXjmjzyCD3/ABrsLDxrZXuo6NYrG4m1axF7ER0VcZwfesD4neKdPl8KXGh6XcR32p6oBBDBbsHIyRljjoBWSYIvC/xG8E2uoyrCkGjtbGVzhd4U8Z/Cls9Ct1qd3f8Aiy2sfFUOgeRNLeTWr3KbBwQuePrxXF+A/G97KviS41iyvjDa3EkvmyL8saqf9X9ef0p0+p2eo/tEaYLGdLgQ6VKrtGcqG5OM+vIqhodzAPCvju1M0YuPtc7eUWG7GeuOuKV9QsrG/H8WrSfTk1WDRNSk0jAMl8sfyp68dwDwfpXQXHjLTYNR0W1j3Tf2zu+zyJ93jHX865TRfEugJ8D0P2y3VV0toXhLDcZPLKkbeuS3865mzik0aL4ZS6sfs6I8wZpDgJllIBPbii7CyPVtX8U2uka/pukzRSSXGoh/J2Dj5QTj9K474f8AirU9V8c+ILS+s74xfacI8i/LbgIPlPpn+tR+J9Wsr740eDreyuY7hoTMZDE24LlDgZFWfh9cwRePPF9rJMiTveqyxMwDMPLXkDvTvqFtDZ+KcjxfDPWXidkcQcMpwRzWz4WYv4S0pmJZjaRkknr8orE+K/Hwv1r/AK4f1qbwx4l0SLwppaSatZq62sYZTOoIO0e9P7RPQn8R+MrfQNQt9Ois7nUdRuULx2tsuW2j+I+g4NU9H+IUGpajd6Zd6XeadqdtAbg2s68ug7g1h6/4gvdQ+IkOkaHe2OlgWYlbUZ0V2kUk8JnjH41i+Gzs+PgguNcbWpBpMivOVAUEsvyjBIpX1HZWPStD8V6frvhb+3rYslqqOzh+qbM7gfpg1lS/EjTIvD9lqYtrmR9Rdks7WNcyTkHHA9K871qW68OatrngezDj+3LpJbNl6JHIwMoH0BatHxlo8ukeO/CkcWpHR7KKzNrHeCMOqS+4JAGcjmlzMfKjuNH8e29/rSaRqmm3ekX8yF4YrpcCUDrg+vI4rOT4qWd1rUumabpF/ezQXRtp2iTKxYbbuJ9M1gXGlwyeP/D6aj4zn1e+ikaW3igtVIUYwdzBjgc1q/CaJFuvFrhRubWpsn1+Y07vYLK1z0ZTlQcYyOhpaKKszCiiigAooooAKKKKACiiigDndW/5KP4H/wCv66/9Ipq9GrznVv8Ako/gf/r+uv8A0imr0auap8R00/hCiiiszQKKKKACiiigAooooAKKKKACiiigAooooAKKKKAPHPhj/wAhbx7/ANjXe/8AoQrv64D4Y/8AIW8e/wDY13v/AKEK7+uqHwo5Z/EwoooqyAooooAKKKKACiiigArO1fQdL1628jWLGG7jHTzEyR9D1FaNFIZhaN4M8PeH5TLpOlW8Ev8Az027mH0J5FW9Z8P6V4gt1h1iyiukQ5XeOVPseorSoosguzI07wromkzwzadp0NvLCrKjovIB6896y/EHg3TZbHVr3TNNiXVbu3eMyr8pfPr27Vdm16SXxjDolgqv5UPn3jnny1JIUD3JBrE134iw2Xiqw0PTLdrh5bv7PdTkfJCdrHbn+9x+lLQauL4X+Hmi2+g6XJq2jW/9owQRiTK5G8KOSOhPHWup1XRNN1ux+x6pZxXNuOiOv3fp6Vfqhd65plhcLBeX0MMrdEd8GnZIV2ynYeDtA0uaCWx0yCGW3YtHIB8wJGM56nrU3/CNaP8A28NaFhENRAx9oAw2MY/lV5722SSGN50DznEQ3ff4zxUV5qdpYrKZ5kVoozKyk87fWjQNR9/YWuqWMtnfwrPbzLteNujCud/4Vl4O/wCgDa/98mrPhzxjp+veHYNVaSO2WZipRpM7Tk8Z/CtWy1jTtS3/AGG8inMf3wjZxRox6ooX3g3QNRgt4rzTIXW1XZCRkMi+gI5xT7bwvo2mSx3On6ZBFPboyxtGu089RnvnA61ore2z2jXSzoYFBLSA8DHXn8KwdT8bWGneKNK0bcjnUEeTzd+AgXbjjvnd+lGgamJpOlar4k+IUfiPXtKGnQ6ZC0FpG7BmdjwWOO2M12up6TYazZta6paxXUDdUkXP/wCqs7Tdcmk8T3+iagqLNComt2XgSRHHP1BIFbtCBmNo/hPQ9AmaXSdOhglcYMgGWx6ZPNXbDSrLTDObC3SE3Ehll2/xsepNXKKBBRRRTEFFFFABRRRQAUUUUAFFFFAHO6t/yUfwP/1/XX/pFNXo1ec6t/yUfwP/ANf11/6RTV6NXNU+I6afwhRRRWZoFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAHjnwx/5C3j3/sa73/0IV39cB8Mf+Qt49/7Gu9/9CFd/XVD4Ucs/iYUUUVZAUUUUAFFFFABRRRQAUUUUAFFFFAHCeDvn+IvjJpDmRbmNFz2Ty1I/DJNL8QYkiv8Awx5aKu7WVZsDGSY5Oa1P7EuNP8ff2xYoGtr+AQ3i5xtZSSr/AI7sfhW1faVZak9s99AJWtZRNCSSNj4Izx7E1NtLFX1uXK8s8G6TomtaXr954kt7W51Br2VLhroAuigDaOegr1Oud1LwJ4d1a/e8vdPzPJ/rGjmkjEn+8FYA/jQ0CZ5b4fuJDN4BluZWNvFqE8cMsp6xiNwnJ9q63UWs9S+LF9bt5Vyi6JslQ4YA7mOCPxrsNR8K6LqmkxabeWEbWsBBiRCU8sgYG0qQRTNK8I6Hotx9o06xWOcx+U0rSM7Muc4JYnPXvS5WPmR5G2n6QfhN4YjnSCKGbVYxdMuBnl/vV1GvWNjpPxG8Or4bght5LiOWO6jtVADQiNiNwHbcFxVzxV8ObObTLOz8P6cqxNqMdxdRmZtpUbskBjx16DFdRo3hHRdAuHn0yz2TONpkkleRgPQFicD6UrMfMef6bf20XwE1tJZ40ZI76NlLDO4vJgY9eahTT9Ol8VeAZbq2t2Munz7mkQfMVMe3OfTJx9a7qb4d+F7i8nuZdLVmuCTKnnSBGJ6nYG25/Crl94R0PUYrKO8sFdbD/j22yOpi+hBB7CnysXMjn9dLJ8YfDZh+89tMsuP7mGP88V3Vc1YaPcz+N73XdQj2LHELWzQnOE4LN+JH5GulqkJhRRRTJCiiigAooooAKKKKACiiigAooooA53Vv+Sj+B/8Ar+uv/SKavRq851b/AJKP4H/6/rr/ANIpq9GrmqfEdNP4QooorM0CiiigAooooAKKKKACiiigAooooAKKKKACiiigDxz4Y/8AIW8e/wDY13v/AKEK7+uetfhfqWl6prF1ofi6axj1bUJr+WE2EUu15GyQC3OBwPwq3/whHij/AKHuX/wVQVvGokrGMqbbua1FZP8AwhHij/oe5f8AwVQUf8IR4o/6HuX/AMFUFP2kSfZyNaisn/hCPFH/AEPcv/gqgo/4QjxR/wBD3L/4KoKPaRD2cjWorJ/4QjxR/wBD3L/4KoKP+EI8Uf8AQ9y/+CqCj2kQ9nI1qKyf+EI8Uf8AQ9y/+CqCj/hCPFH/AEPcv/gqgo9pEPZyNaisn/hCPFH/AEPcv/gqgo/4QjxR/wBD3L/4KoKPaRD2cjWorJ/4QjxR/wBD3L/4KoKP+EI8Uf8AQ9y/+CqCj2kQ9nI1qKyf+EI8Uf8AQ9y/+CqCj/hCPFH/AEPcv/gqgo9pEPZyNaisn/hCPFH/AEPcv/gqgo/4QjxR/wBD3L/4KoKPaRD2cjWorJ/4QjxR/wBD3L/4KoK4zT77XdQ+KV94Ph8bPutLcSGcabAdz9SoHoFo9pEPZSPSaKyf+EI8Uf8AQ9y/+CqCj/hCPFH/AEPcv/gqgo9pEPZyNaisn/hCPFH/AEPcv/gqgo/4QjxR/wBD3L/4KoKPaRD2cjWorJ/4QjxR/wBD3L/4KoKP+EI8Uf8AQ9y/+CqCj2kQ9nI1qKyf+EI8Uf8AQ9y/+CqCj/hCPFH/AEPcv/gqgo9pEPZyNaisn/hCPFH/AEPcv/gqgo/4QjxR/wBD3L/4KoKPaRD2cjWorJ/4QjxR/wBD3L/4KoKP+EI8Uf8AQ9y/+CqCj2kQ9nI1qKyf+EI8Uf8AQ9y/+CqCj/hCPFH/AEPcv/gqgo9pEPZyNaisn/hCPFH/AEPcv/gqgo/4QjxR/wBD3L/4KoKPaRD2cjWorJ/4QjxR/wBD3L/4KoKP+EI8Uf8AQ9y/+CqCj2kQ9nIp6t/yUfwP/wBf11/6RTV6NXEaf4C1OPxPpWr6x4ol1MaZJJJDAbGOIFniaMkleejmu3rGTu7m0VZWCiiipKCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAIyCM4968d8N/Diw8O/tBTXsGoX1xK2mPfnzmUgvJI0ZXgDgDpXsVccv/ACXGT/sXE/8ASlqAOxooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigArjl/5LjJ/2Lif+lLV2Nccv/JcZP+xcT/0pagDsaKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAK45f+S4yf9i4n/pS1djXHL/yXGT/ALFxP/SlqAOxooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAorkp/ih4Rt766s31OV57SZ4J1isZ5BHIpwykqhGQRTf+FqeEf+f+6/8ABZdf/G6AOvorkP8AhanhH/n/ALr/AMFl1/8AG6P+FqeEf+f+6/8ABZdf/G6dmB19Fch/wtTwj/z/AN1/4LLr/wCN0f8AC1PCP/P/AHX/AILLr/43RZgdfRXIf8LU8I/8/wDdf+Cy6/8AjdH/AAtTwj/z/wB1/wCCy6/+N0WYHX0VyH/C1PCP/P8A3X/gsuv/AI3R/wALU8I/8/8Adf8Agsuv/jdFmB19Fch/wtTwj/z/AN1/4LLr/wCN0f8AC1PCP/P/AHX/AILLr/43RZgdfRXIf8LU8I/8/wDdf+Cy6/8AjdH/AAtTwj/z/wB1/wCCy6/+N0WYHX0VyH/C1PCP/P8A3X/gsuv/AI3R/wALU8I/8/8Adf8Agsuv/jdFmB19Fch/wtTwj/z/AN1/4LLr/wCN0f8AC1PCP/P/AHX/AILLr/43RZgdfXHL/wAlxk/7FxP/AEpanf8AC1PCP/P/AHX/AILLr/43XML4/wDDw+K76t9ou/sJ0RbYTf2dc480Ts23Hl56HOcYpWYXPVqK5D/hanhH/n/uv/BZdf8Axuj/AIWp4R/5/wC6/wDBZdf/ABunZgdfRXIf8LU8I/8AP/df+Cy6/wDjdH/C1PCP/P8A3X/gsuv/AI3RZgdfRXIf8LU8I/8AP/df+Cy6/wDjdH/C1PCP/P8A3X/gsuv/AI3RZgdfRXIf8LU8I/8AP/df+Cy6/wDjdH/C1PCP/P8A3X/gsuv/AI3RZgdfRXIf8LU8I/8AP/df+Cy6/wDjdH/C1PCP/P8A3X/gsuv/AI3RZgdfRXIf8LU8I/8AP/df+Cy6/wDjdH/C1PCP/P8A3X/gsuv/AI3RZgdfRXIf8LU8I/8AP/df+Cy6/wDjdH/C1PCP/P8A3X/gsuv/AI3RZgdfRXIf8LU8I/8AP/df+Cy6/wDjdH/C1PCP/P8A3X/gsuv/AI3RZgdfRXIf8LU8I/8AP/df+Cy6/wDjdH/C1PCP/P8A3X/gsuv/AI3RZgdfRXM6Z8Q/DOsatb6ZYX8rXlyWEMctnPFvKqWOC6AcKpPXtXTUgCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA8c+GP/IW8e/9jXe/+hCu/rgPhj/yFvHv/Y13v/oQrv66ofCjln8TCiiirICiiigAooooAKKKKACikJABJ4A6mkR1kUNGwZT0KnINADqKZ5qeZs3rvxnbnnHrilSRJF3Rurj1U5oAdRTXkSMZkZVBOMscU6gAopokQuUDKWHJXPIp1ABRRRQAUUlIkiSLmN1cdMqc0AOooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAOd1b/ko/gf/AK/rr/0imr0avOdW/wCSj+B/+v66/wDSKavRq5qnxHTT+EKKKKzNAooooAKKKKACiiigAooooAKKKKACiiigAooooA8c+GP/ACFvHv8A2Nd7/wChCu/rgPhj/wAhbx7/ANjXe/8AoQrv66ofCjln8TCiiirICiiigAooooAKKKKAKGuEjw7qJHBFrLj/AL4NcN8L/FWi2vw10qDUNZs4rlBKHSW4UMP3r9QT6YruNd/5F3Uv+vSX/wBANeffDDwX4c1P4b6VeahpFtPcSiUvI6nLYlcfyFS73LVrakdtq9nqXx4u5LK8S7tBobbjBKGXIIz071peGvFvhzQvh2+q2kV3BZLdSokMzeZLJJ5hGF5Oct0rFtdHsNF+Ol9baVax2sJ0Fm2RjAzkc1ytohi+Gfh/UJVJsrHxFJLdHHCp5rAE+wJFRdoqyZ0HjnxvcarpumWl3ouoaRJNfwyQNPtxIobnlScH2OK7jXfHsWkatFo2m6ZdavqPlCSSG12/ul9WLED9a5v4q+INGvdG0W3tbqC4uJ9Qhkh2MCQobk1N4dvrXRPiz4jt9ZlSCe/SKa0eU43RjOQD+Ip9RdCHwZ4jj1j4reI76RJ7OOKwj82G5G0wldu7PbseRWk3xUEiS3lh4d1O70mFir30artIHVlUncR9BXNSXMHiHxv4+Hh6RZnfRvKDxc+Y4UAgY754ql4Xjl/4V7FOfiDPYWtvEUntPLh/ckE5Ugpn86V2OyPQ9b+JOj6NpOlakFmvLXVHCQPbrk5KkjIPPbH41BpnxIW58Q22lavol9pD3mfsslyF2ye3yk4Psa4FLCztPDngKGxkuLi0fU/Mja5TaxUxuRxjpXX/ABP48TeByOv9rYz+Ap3e4rLY9Buf+PSb/cb+VcH8GZHk8EytIzMft04yxz/Ga7y6/wCPOb/rm38q8t+EvibRNK8IzW2paraWs4vpyY5ZQrYLnsap7krY77xN4msvCukm+v1kky4jjhiXc8rngKo9a5uD4mNFqtnaa74d1DSYr59kFxNsZCewO1jg1X8e+NYE03Sx4fudPuDfXy2wvZMSR2p4+Y+hGc1xXjmO5tbzQotU8Xvq909/G62qqixovOWwBnuO9JsqMe567o3iq21jXtU0gW81tdaa4DrLj94p6OuD06VQb4h6TFNrf2hZYrbRiFmumAKSORnauOSeR2rmfiDfnwN4ztfFkSkW95bSWlzgfecITH+qqKwPEnhy7svgYlwwkFxcXa6hfFRlsFs5/BcUNsEkdlD8UQj28+reHtS03Tblwsd7MqlQT03AEkZ9xVrxH8SbPQNc/siLTbzUL14BNClsobzQew57Dn6CuI8QW8U3hFG1T4iXV9YXexEt4o4WMhyMAAJn8q29Ktlg+M+nxHc5g0NFVpBhuFAyfQ0rsLI9E0i/fVNIt72W1ls3mXcYJsb056HFXaKKsgKKKKYgooooAKKKKACiiigDndW/5KP4H/6/rr/0imr0avOdW/5KP4H/AOv66/8ASKavRq5qnxHTT+EKKKKzNAooooAKKKKACiiigAooooAKKKKACiiigAooooA8c+GP/IW8e/8AY13v/oQrv64D4Y/8hbx7/wBjXe/+hCu/rqh8KOWfxMKKKKsgKKKKACiiigAooooAZLGk0LxSqHR1Ksp6EHqKisLC10yyjs9PgS3t487IoxhVycn9SasUUhlI6Rp7ao2otaRG8aLyWn2/MU/u59Kbb6Hplppr6fbWMMdpIWLwqvynccnj3zV+sA69JP4wl0q0CmCxg828kxkgkfKg98EGgNRLbwJ4Xs932bRLSPc4c4j/AIhyDVvWPDWja+iLrOnQXYj+55i52/SuJk+IHiH+w7nxPFp9idDt5WUxNIfPZFOC3png8V6La3C3VrFPHnbIoYZHrSVmN3RT07w/pOkSGTTNPgtXZBGWjTBKjoKo3PgTwveagb250S0kuCdxkMfJPqatz+J9CtXC3Gs2EbMxQBrlAdwOCOvY8Vbu9SsrGz+1Xt3BBb9fNlkCqfxPFPQWoy40jT7s2xubOKQ2rb4Ny/6s4xkenBNOvNLsdRmtpb61jnktZPMgZ1yY29R70211jTb6za7s7+1nt05eWOZWVfqQcCpXvrSK1W6kuoUt3xtmaQBGz0wenNAak7AMpVhkEYIrm3+HfhGSRnfw9YszHLExdTWrc6/pFlIY7vVLOGQEApJcKpGenBPfFNvPEOjafdLbX2q2VvO3SOW4VWP4E0aBqVk8HeHk0mTTE0e1FjI+94Anys3rj8KjtvA/hm0tfs8Gi2iRbw5XZ/EOh/WtS51SwskR7y+trdZAWQyyqoYAZJGTzxTbDWdN1WJ5NNv7a7SP77QzK4X64PFGganE+MNM13xd4htdCfSlg0SC4S4mv2kz5oX5tgXHqAK777PEbYW5jUxBdmwjjHTFUrXxHot7eG0s9Wsp7gceVHcIzfkDVK112ZPGVzoeoKq74hcWbr/GnQqfcEE/QiloPUS08C+GLHUBe2miWkVyDkSLHyD7Vq/2XZf2p/aP2WP7Zs8vz9vzbfTPpVuimK7CiiimIKKKKACiiigAooooAKKKKAOd1b/ko/gf/r+uv/SKavRq851b/ko/gf8A6/rr/wBIpq9GrmqfEdNP4QooorM0CiiigAooooAKKKKACiiigAooooAKKKKACiiigDxz4Y/8hbx7/wBjXe/+hCu/rgPhj/yFvHv/AGNd7/6EK7+uqHwo5Z/EwoooqyAooooAKKKKACiiigAooooAK888GXEsN343unTzLmK9dgp6sFQ7R+QFeh1zUGi3Gm+O7jUbRA1lqUQFyoONkijAbHuBj8aTKR5kvhm0vPhpd+JZNWliuHaS7a0DgW4dWPyFOuOORmvYPD17JqPhywvJ4hDJNArtGBjaSOlZUvw78MTag13JpUJd38xkx8jN6ketdMqhVCqMAcACklYG7nknhTQNMvPB/jC5u7SOaaS9v/ncZK4d8Y9OnasnfqV5N4JtcW80DWkrRpek+XJIGxg8jJAxivY7TRNPsbO5tbW2SOG6d3mQDhy5JYn65NVb3wnouoaPDpd1YRPaQcxJj/Vn1HpS5R82p5ubC+tdW8TCeXT4Wk0hzLY2PADcYfGTjjP51Y8Sahan4IaIomUtIbJFUHkkOmfyr0HR/Cuj6FbzxabZRxi4/wBcxGTJ9T3rOh+G3hS3maSPSIcltwBXIQ5zwO3NHKw5kcno2jWOq/FbX5L61S4aCwtvKDjIBO/nH4CpfBljpOpeHfENx4gihm1A3lyt202C0YDNtAz049K9CttHsLPUp7+2t0jubhFSWQDlgucD8MmsrU/Afh3V9Qa9vtOjeaT/AFhAx5n+960WC55hptuNXi8Bw6mpnt/tl0I95Pzxq5259RgCugvrGxsPiRrNrAy6daT6OGlMQ2qpyRux+NehNoemvLZSG0jDWGfs2F/1XGOPSnPo1hLqUl/JbRtcyReS8hHJT+79KOUOY8itFm8N2ugPrGm6bqWmrNFHaalYN5UxLYVWYZJbOea7PxYdvxE8Hvb8TNJOr467Nq9f1rTsvh94Z0/UEvLXTIlkibdGMfLGfVR2og0a6u/Hk2tX6BIbSH7PZLnOc8s/45x+FFgujpaKKKsgKKKKACiiigAooooAKKKKACiiigDndW/5KP4H/wCv66/9Ipq9GrznVv8Ako/gf/r+uv8A0imr0auap8R00/hCiiiszQKKKKACiiigAooooAKKKKACiiigAooooAKKKKAPHPhj/wAhbx7/ANjXe/8AoQrv6yY/hTp9tqOpXmm6/rtgdSvJL24jtriIIZZDliAYyf1qb/hXH/U3+Jv/AAJh/wDjVbRqJKxjKm27mhRWf/wrj/qb/E3/AIEw/wDxqj/hXH/U3+Jv/AmH/wCNVXtUL2TNCis//hXH/U3+Jv8AwJh/+NUf8K4/6m/xN/4Ew/8Axqj2qD2TNCis/wD4Vx/1N/ib/wACYf8A41R/wrj/AKm/xN/4Ew//ABqj2qD2TNCis/8A4Vx/1N/ib/wJh/8AjVH/AArj/qb/ABN/4Ew//GqPaoPZM0KKz/8AhXH/AFN/ib/wJh/+NUf8K4/6m/xN/wCBMP8A8ao9qg9kzQorP/4Vx/1N/ib/AMCYf/jVH/CuP+pv8Tf+BMP/AMao9qg9kzQorP8A+Fcf9Tf4m/8AAmH/AONUf8K4/wCpv8Tf+BMP/wAao9qg9kzQorP/AOFcf9Tf4m/8CYf/AI1R/wAK4/6m/wATf+BMP/xqj2qD2TNCis//AIVx/wBTf4m/8CYf/jVcfYRWOofE688IQeLvEpktbYSNJ9qh+aTqVH7r+7zR7VB7JnoFFZ//AArj/qb/ABN/4Ew//GqP+Fcf9Tf4m/8AAmH/AONUe1QeyZoUVn/8K4/6m/xN/wCBMP8A8ao/4Vx/1N/ib/wJh/8AjVHtUHsmaFFZ/wDwrj/qb/E3/gTD/wDGqP8AhXH/AFN/ib/wJh/+NUe1QeyZoUVn/wDCuP8Aqb/E3/gTD/8AGqP+Fcf9Tf4m/wDAmH/41R7VB7JmhRWf/wAK4/6m/wATf+BMP/xqj/hXH/U3+Jv/AAJh/wDjVHtUHsmaFFZ//CuP+pv8Tf8AgTD/APGqP+Fcf9Tf4m/8CYf/AI1R7VB7JmhRWf8A8K4/6m/xN/4Ew/8Axqj/AIVx/wBTf4m/8CYf/jVHtUHsmaFFZ/8Awrj/AKm/xN/4Ew//ABqj/hXH/U3+Jv8AwJh/+NUe1QeyZoUVn/8ACuP+pv8AE3/gTD/8ao/4Vx/1N/ib/wACYf8A41R7VB7Jmbq3/JR/A/8A1/XX/pFNXo1chp/w7tbLxBp+r3Gu61qM2nu7wR3k8bIrPG0ZOFjB+657119Yyd3c1iuVWCiiipKCiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAIyCP5V5NoXw/wBF0D4/S3dj9qMx0lr4tLOWzJJK0bfht6CvWa45f+S4yf8AYuJ/6UtQB2NFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFccv8AyXGT/sXE/wDSlq7GuOX/AJLjJ/2Lif8ApS1AHY0UUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVxy/8AJcZP+xcT/wBKWrsa45f+S4yf9i4n/pS1AHY0UUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUVy178TfBenX01nfeJLGC4t5GilieTBRlOCD7ggioP+Fs+A/+hq07/v7QB2FFcf8A8LZ8B/8AQ1ad/wB/aP8AhbPgP/oatO/7+0AdhRXH/wDC2fAf/Q1ad/39o/4Wz4D/AOhq07/v7QB2FFcf/wALZ8B/9DVp3/f2j/hbPgP/AKGrTv8Av7QB2FFcf/wtnwH/ANDVp3/f2j/hbPgP/oatO/7+0AdhRXH/APC2fAf/AENWnf8Af2j/AIWz4D/6GrTv+/tAHYUVx/8AwtnwH/0NWnf9/aP+Fs+A/wDoatO/7+0AdhRXH/8AC2fAf/Q1ad/39o/4Wz4D/wChq07/AL+0AdhRXH/8LZ8B/wDQ1ad/39o/4Wz4D/6GrTv+/tAHYVxy/wDJcZP+xcT/ANKWpf8AhbPgP/oatO/7+1yy/Enwd/wt2TUv+EisfsR0NbcT+Z8vmeezbfrg5oA9Zorj/wDhbPgP/oatO/7+0f8AC2fAf/Q1ad/39oA7CiuP/wCFs+A/+hq07/v7R/wtnwH/ANDVp3/f2gDsKK4//hbPgP8A6GrTv+/tH/C2fAf/AENWnf8Af2gDsKK4/wD4Wz4D/wChq07/AL+0f8LZ8B/9DVp3/f2gDsKK4/8A4Wz4D/6GrTv+/tH/AAtnwH/0NWnf9/aAOworj/8AhbPgP/oatO/7+0f8LZ8B/wDQ1ad/39oA7CiuP/4Wz4D/AOhq07/v7R/wtnwH/wBDVp3/AH9oA7CiuP8A+Fs+A/8AoatO/wC/tH/C2fAf/Q1ad/39oA7CiuP/AOFs+A/+hq07/v7R/wALZ8B/9DVp3/f2gDsKK5zSfiF4S13U4tP0fX7K8vJs+XDE+WbALHH4An8K6OgAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKAPGvhpGkmr+PC6KxHiq96jP8QrvPs8P/PGP/vkVwvwx/wCQt49/7Gu9/wDQhXf11Q+FHLP4mRfZ4f8AnjH/AN8ij7PD/wA8Y/8AvkVLRVkkX2eH/njH/wB8ij7PD/zxj/75FS0UARfZ4f8AnjH/AN8ij7PD/wA8Y/8AvkVLRQBF9nh/54x/98ij7PD/AM8Y/wDvkVLRQBF9nh/54x/98ij7PD/zxj/75FSEhVJY4A5JPakjkSVA8Tq6noynINADPs8P/PGP/vkUfZ4f+eMf/fIpfOi80x+YnmAbim4Zx64pY5Y5l3ROrrnGVORQA37PD/zxj/75FH2eH/njH/3yKdJLHEoMrqgJwCxxzT6AIvs8P/PGP/vkUfZ4f+eMf/fIpwljMhjDqXUZKg8j8KfQBF9nh/54x/8AfIo+zw/88Y/++RUtFAEX2eH/AJ4x/wDfIo+zw/8APGP/AL5FSdKbHLHMu6J1dc4ypzQA37PD/wA8Y/8AvkUfZ4f+eMf/AHyKlooAi+zw/wDPGP8A75FH2eH/AJ4x/wDfIqWigCL7PD/zxj/75FH2eH/njH/3yKlooAi+zw/88Y/++RR9nh/54x/98ipaKAIvs8P/ADxj/wC+RR9nh/54x/8AfIqWigCL7PD/AM8Y/wDvkUfZ4f8AnjH/AN8ipaKAIvs8P/PGP/vkUfZ4f+eMf/fIqWigCL7PD/zxj/75FH2eH/njH/3yKlooA5vVI0j+I/gfYirm+us4GP8Alymr0ivOdW/5KP4H/wCv66/9Ipq9GrmqfEdFP4QooorM0CiiigAooooAKKKKACiiigAooooAKKKKACiiigDxz4Y/8hbx7/2Nd7/6EK7+uA+GP/IW8e/9jXe/+hCu/rqh8KOWfxMKKKKsgKKKKACiiigAooooAoa4SPDuokHBFrLgj/cNcD8MfGmgWPw40q21PWraK6jEokSWX5h+9cjP4Yrvdd/5F3Uv+vSX/wBANcB8L/BvhvU/htpV5qGh2NzcyiUvLLArM2JXHJ+gqHe+hatbUrWWt2Or/Ha8m069W7s10NtxhfK5BGfxrU8N+LvD+hfDt9Vt4rmK0+1SxxwyP5kkkhkIwOB1NY9tpOn6N8db620mzhs4P7BZvLhQKucjnArlLSN0+Gfh/UHjZ7Sw8QyTXRAztTzWG4/QkGpu0VZM6Px74zvNS0zS7W90S+0iSe/heF5DlZFDcjOBg+1drrPjtNM1qPRNL0u51fUREJZYrc4ES+pODXN/FPxFo1/ouj29ndw3M82oQSR+UwbaobnOOlTeH7+10D4reIoNalS2k1BIprWWZtqsgzkAnjuOKfUVtCHwd4iTVfit4kvZ457KOHT4/NhuODEV27v5HmtX/hZ7TW8l/p/hvULvSY2IN8nAIHVguOR+NcxJcw+I/G/j0aC4maTRvKWROkrBAMA9/SqvheKBfh/DNP49urCO2iKT2RwGiIzldu7J/wDr0XY7I77WviVpOk6RpWpxxy3ltqjhITD1yVJHH4Y/Go7D4iF/EFppWt6FeaQ19kWsk5ysh9Ogwa4EWFpZeH/AcNj9pa1bVDJH9rQK+DG5GQCa634n8eJvA+P+gt/QUXYrLY9Buf8Aj0m/3G/lXB/BmR5PBMrSOzn7dOMsc/xmu8uv+POb/rm38q8s+E3ijRNK8Iz22panb20wvpyY5HwcFzVPclbHf+JvEtp4X0r7ZeJLKWcRxQwrl5HPAAFc8nxIlttTsrbX/Dt7pUN6/lw3Ejbl3HoDwMVU8d+No003S/8AhHbu0kF5fLbG+cb47U8fMfzrjfHaNbXGhpd+L5NbuH1CNvJQARIOeSASAeeM+9JvsUo9z1nR/FMGra/qmjmB7e605gGVznzFPRh7ciqD/ETSoZNce5V47TRmCS3OciRyM7VHryK5r4h3zeB/GNp4thU+Rd20lnc7Rnc4QmP/AMeVRWD4j8NXdp8DYpdshuLi8XUL3yxlvmbOffC4obYJI7RfiY8AgudY8OX+nabcOFS8k5UZ6bhjjP1qx4h+I9vomvro1tpd1qV7LAJoIrbkyg9hx6c1xevW2nT+Ela+8fXuo2l3sRbSFQ7yEkYAXd7d/StjSrZIPjPYQgMfJ0NEUyD5uFA596V2FkeiaVey6jpVvd3FpJZySrlreX70Zz0NXKKKsgKKKKYgooooAKKKKACiiigDndW/5KP4H/6/rr/0imr0avOdW/5KP4H/AOv66/8ASKavRq5qnxHTT+EKKKKzNAooooAKKKKACiiigAooooAKKKKACiiigAooooA8c+GP/IW8e/8AY13v/oQrv64D4Y/8hbx7/wBjXe/+hCu/rqh8KOWfxMKKKKsgKKKKACiiigAooooAbIiyxtHIoZHBVlPQg9qis7O30+0S2soUggjztjQYC5Oen1NT0UhlQ6ZZNqDXxtozdNH5Rm2jcU/u59KbDpGn29g9lDZwrauSWhCDa2Tk5FZ3inXZdIt7W3sAr6hfTiC2RhkZ6liPQAfqKzfHHjgeEdKdYbd77UzAZEijT5QAOXb0Aouh2bNS18F+HLPd9m0a0Tc4cnyh1HT6Va1bw9pOuRomrafBdBPumRASv0PWsvUfEl7aeC7TU7SxN5fXaRiOFTgb3Hc84ArMg8S+IdH8SaZp3iiG0ki1QlIZbXIMbgZwQc5GAeaWganVWGiaZpbl9OsYLZigQmKMLlR24qjceCvDd3qH2240azecnJYxDk+pHQ1r3V1BZW7T3UqxRJ952OAKoW3iXRby/wDsVrqVvLc4z5atyaegtS3PpllcmAz2sT/Z23Q5UfuzjGR6daW60+0vZYJLu3jme3ffEzrko3qPQ1V1DxFpGlTpDqOoQ28j/dV25NWX1KzjWAvcxgXJCwnd98+goDUskBlIPIIwawX8D+GJJGd9DsWZjkkwjk1rvf2sd4LR50WcoZBGTztHU1TtvEmjXtz9ntdRt5ZdnmbFfJ2+tGgajB4V0JdMk05dKtRaSNuaERDaT649eKZb+D/D1pb+TBpFqqbg/wDqwTkdDnrUtp4m0a/vjZ2epQS3AOPLVuaL3xNo2nO63upQQtG21wzcg+lGganKeL9I17xbr1roz6dFDolvcJcS3jOCZAvzbQuc9QBXeeRF9n8gxqYtuzYRkY9MVVl1nTodNGoS3kK2jAETFvlOaittYtNasZn0K+gmkUEK4+YK3bI4oHqVrXwX4csr/wC2WujWkc4OQ4iHyn2HatP+z7T+0Pt32eP7Vs2edt+bb6ZrN8K662u6QZLlFivLeRoLqJeiSL1/CtujQWoUUUUxBRRRQAUUUUAFFFFABRRRQBzurf8AJR/A/wD1/XX/AKRTV6NXnOrf8lH8D/8AX9df+kU1ejVzVPiOmn8IUUUVmaBRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQB458Mf+Qt49/7Gu9/9CFd/XAfDH/kLePf+xrvf/QhXf11Q+FHLP4mFFFFWQFFFFABRRRQAUUUUAFFFFAHB+IiT8YPDCzcRC1uTH7vmOtnx8i/8IFrb4G77FIM4/2TR4t0KfU1sL/TQv8AaOm3Amhycbx0ZSfQ5z+Fbdzaw6jYPbXsIeGZNskbHqD1FTbcq+xzsfiGx8L/AA5sdT1N9sUdpHhR1dtowo9zWR4ctW8ReIbfxL4ku7ZLiMEafpyTK3kA9zg8tj+tdZqXhnR9X0yLTtSskntIceXEWYBcdOhqlY+A/DWm30V3ZaYsU8JyjiWQ4P0LYoswuin8URu+G+rD1i/rXO6/oem6VZeC5tOsoLeVb+3AkjQK2GAJ5969Hv7C11OyktL+FZreQYdGJwfyplzpNjdx2yXNusi2jq8AJPyMvQihq4J2PJdOstR1DxV4sklXRHkF48bjUy3mLCCduPlPy4qO2jGnaL4UWbUre7to9acJNDuEcYL/AHQWA4B4HbivT9U8H6DrN39q1HT1lnxgusjoWHodpGfxqe58N6PeaOmlXGnwtZJjZCBgLjuCOQfelyj5jjNQvLa5+MkaW88crR6LPu2MDjJHpXM6Ro9rZfAGbUbC0Rb+ZF825RB5hQyrvG7rjaT+Fep6d4Q0HSZxNp+nRxShGj8zczMVbqCSST0q7Z6RYWGmDTrS1SOzClRD1XB7c0cocx5r4ws9Ds/Aeiz+H47VNQjmtjYtb48wsSM4xzz3q14Z0PT9T8beLbjU7OG5mUxIDKgbblWzjPTOB+VdfY+CvD2nXy3lnpkaTocoS7MEPqqkkD8BWnb6ZZ2l1c3FvAsc12QZ3BPzkZxn8zRbUObQ8asdMt73wPY2Q1G3sJrXXbn7DHdIWhkKyOAjcEAY6fQV0/gq7Nt48vdO1LSbO11OS1WRrnT5N0UyAtjI4weD2rsZfCmiTaa+nyadE1q8rTGPJ++xyWBzkHJ7Umn+HdM8PQTyaJp6rMy85kZmfHQbmJNCjYHK5g+CiR428ZKn+pF3ERj+8Q+7+ldvWB4Q0ObRtJka/Ktf3kzXF0y9N7dvoK36pbEvcKKKKYgooooAKKKKACiiigAooooA53Vv+Sj+B/8Ar+uv/SKavRq851b/AJKP4H/6/rr/ANIpq9GrmqfEdNP4QooorM0CiiigAooooAKKKKACiiigAooooAKKKKACiiigDxz4Y/8AIW8e/wDY13v/AKEK7+uetvhfqOl6pq91ofi6exj1XUJb+WE2EUoV5GyQC3OOg/Crf/CEeKP+h7m/8FUFbxqJKxjKm27mtRWT/wAIR4o/6Hub/wAFUFH/AAhHij/oe5v/AAVQU/aRJ9nI1qKyf+EI8Uf9D3N/4KoKP+EI8Uf9D3N/4KoKPaRD2cjWorJ/4QjxR/0Pc3/gqgo/4QjxR/0Pc3/gqgo9pEPZyNaisn/hCPFH/Q9zf+CqCj/hCPFH/Q9zf+CqCj2kQ9nI1qKyf+EI8Uf9D3N/4KoKP+EI8Uf9D3N/4KoKPaRD2cjWorJ/4QjxR/0Pc3/gqgo/4QjxR/0Pc3/gqgo9pEPZyNaisn/hCPFH/Q9zf+CqCj/hCPFH/Q9zf+CqCj2kQ9nI1qKyf+EI8Uf9D3N/4KoKP+EI8Uf9D3N/4KoKPaRD2cjWorJ/4QjxR/0Pc3/gqgri9Pvtd1H4p33g6LxtJutLcSGcabAdz9SoHoFo9pEPZSPSqKyf+EI8Uf8AQ9zf+CqCj/hCPFH/AEPc3/gqgo9pEPZyNaisn/hCPFH/AEPc3/gqgo/4QjxR/wBD3N/4KoKPaRD2cjWorJ/4QjxR/wBD3N/4KoKP+EI8Uf8AQ9zf+CqCj2kQ9nI1qKyf+EI8Uf8AQ9zf+CqCj/hCPFH/AEPc3/gqgo9pEPZyNaisn/hCPFH/AEPc3/gqgo/4QjxR/wBD3N/4KoKPaRD2cjWorJ/4QjxR/wBD3N/4KoKP+EI8Uf8AQ9zf+CqCj2kQ9nI1qKyf+EI8Uf8AQ9zf+CqCj/hCPFH/AEPc3/gqgo9pEPZyNaisn/hCPFH/AEPc3/gqgo/4QjxR/wBD3N/4KoKPaRD2cjWorJ/4QjxR/wBD3N/4KoKP+EI8Uf8AQ9zf+CqCj2kQ9nIp6t/yUfwP/wBf11/6RTV6NXEaf4B1OLxPpWr6x4pl1MaZJJJDAbGOIFniaMkleejmu3rGTu7m0U0rBRRRUlBRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAEZBGce9eO+G/hxp/h39oKa9gv724lbTHvz5zKQXkkaMrwBwB0r2KuOX/AJLjJ/2Lif8ApS1AHY0UUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVxy/8AJcZP+xcT/wBKWrsa45f+S4yf9i4n/pS1AHY0UUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAVxy/8lxk/7FxP/Slq7GuOX/kuMn/YuJ/6UtQB2NFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFclP8AFDwjb311Zvqcrz2cz286xWNxII5FOGUlUIyCKb/wtPwj/wA/13/4LLr/AON0AdfRXIf8LT8I/wDP9d/+Cy6/+N0f8LT8I/8AP9d/+Cy6/wDjdOzA6+iuQ/4Wn4R/5/rv/wAFl1/8bo/4Wn4R/wCf67/8Fl1/8boswOvorkP+Fp+Ef+f67/8ABZdf/G6P+Fp+Ef8An+u//BZdf/G6LMDr6K5D/hafhH/n+u//AAWXX/xuj/hafhH/AJ/rv/wWXX/xuizA6+iuQ/4Wn4R/5/rv/wAFl1/8bo/4Wn4R/wCf67/8Fl1/8boswOvorkP+Fp+Ef+f67/8ABZdf/G6P+Fp+Ef8An+u//BZdf/G6LMDr6K5D/hafhH/n+u//AAWXX/xuj/hafhH/AJ/rv/wWXX/xuizA6+iuQ/4Wn4R/5/rv/wAFl1/8bo/4Wn4R/wCf67/8Fl1/8boswOvrjl/5LjJ/2Lif+lLU7/hafhH/AJ/rv/wWXX/xuuYXx/4eHxXfVvtF59hOiLbCb+zbn/Widm248vPQ5zjFKzC56tRXIf8AC0/CP/P9d/8Agsuv/jdH/C0/CP8Az/Xf/gsuv/jdOzA6+iuQ/wCFp+Ef+f67/wDBZdf/ABuj/hafhH/n+u//AAWXX/xuizA6+iuQ/wCFp+Ef+f67/wDBZdf/ABuj/hafhH/n+u//AAWXX/xuizA6+iuQ/wCFp+Ef+f67/wDBZdf/ABuj/hafhH/n+u//AAWXX/xuizA6+iuQ/wCFp+Ef+f67/wDBZdf/ABuj/hafhH/n+u//AAWXX/xuizA6+iuQ/wCFp+Ef+f67/wDBZdf/ABuj/hafhH/n+u//AAWXX/xuizA6+iuQ/wCFp+Ef+f67/wDBZdf/ABuj/hafhH/n+u//AAWXX/xuizA6+iuQ/wCFp+Ef+f67/wDBZdf/ABuj/hafhH/n+u//AAWXX/xuizA6+iuQ/wCFp+Ef+f67/wDBZdf/ABuj/hafhH/n+u//AAWXX/xuizA6+iuZ0z4h+GdY1a30yxv5WvLksIY5bOeLeVUscF0A4VSevaumpAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQB458Mf8AkLePf+xrvf8A0IV39cB8Mf8AkLePf+xrvf8A0IV39dUPhRyz+JhRRRVkBRRRQAUUUUAFFFFABRRTI5Y5c+VIr7Tg7Wzg+lAD6KZJKkS7pXVFzjLHFCSxuzBHVivDAHOKAH0UUzzY95TzF3qMld3IHrQA+impIkqBo2V1PQqcikeVIyokdVLHCgnGTQA+iiigAopkkqRLuldUXpljgU6gBaKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigDndW/5KP4H/6/rr/0imr0avOdW/5KP4H/AOv66/8ASKavRq5qnxHTT+EKKKKzNAooooAKKKKACiiigAooooAKKKKACiiigAooooA8c+GP/IW8e/8AY13v/oQrv64D4Y/8hbx7/wBjXe/+hCu/rqh8KOWfxMKKKKsgKKKKACiiigAooooAK818Ko/hX4r6zoMhItNVUX9mM8bsYf8A9Br0quB+J8DaculeK7dT5mjXIebb1aEkbx9MZqX3Kj2K3jyR/EXjfQfCUJJhDm+vdp6ImAo/Et+lWdN8Q+H9EuPFuoR21xb/AGGcG8dn3CVtvG0Z47DtUfw3T+3tU1nxjMuft8ohtN3VIVz0+uf0rBtH0+M/ENtZtpLmy+1KJo4hlsbRyPp1/Cl5leR0Vv8AEa/Sazm1bwvfWWnXrqsN3uR8buhZVYkD8Kfd3Wkt4816CG1nXVF0gNJcGT920ZU4AXPB98VwV9dzeEdHtNR8J+MF1ezkdBFpN3slZ8/wggA8V05Zn+LfiNmXaW0BCR6fKaVwsY/w+8d3+m/D62+x+HL7UbOy3m5uo2QAfMSdoLAtgegrY8Z61ba7ceBNS06Qtb3OpBh2Pbg1B8OPGXh3T/hOqXl7b272qSiaB3AYnLHp3z/WudsrK4s9A8CG5Vk+0a280aMMFUYril0HbU9J1rx5Ja68+ieH9HuNZv4VDzrCyqkQ9CzEDPtV/wAK+L7fxMLmA201jf2b7Lm0nA3xn144IOeorzG3spLX4neJ7W98U3Ph6a6uPtEAXyws8ZzjBdT0z2rofhva6e/jbXL6y1m91mcRpDPdyhPLcjBwCqgEjpVJu4mlYt/G5mX4d5UkH7bD0+prv7b/AI9Yv90V598cWC/DgsxwBeQkn0610Ft4+8J+REn/AAkWm7toGPtC9fzo6k9CHX/Gsun64ui6Lo9zq+o+X5rxxFVWNf8AaZiB2qppfxF+2QatDeaNdWmp6VGJJbJipZ1OeVIJB6etc9qXiC+1v4jalpFprkHh62s4Y2W5SNDLdhlDZDMCMDPp2qj8Pvsc3xZ1yCDVZtXjNkqNdTlSZDk5xtAGOaV9SrKx6NbeMNPufAw8UKSLT7N55UkbgcZ2/XPH1rKvfiNFBYaWLTSrq61TVI/Ng09Nu9VzwzEnAHvmvOpIby31iX4ZRqwhn1UXCccC1yZiM/gBWn4w01tP+LmnyS6zPoVpNpy29teRhMBlLfJlgQOCPzo5mHKjuNC8d/b9Wm0fWtMn0jVI4jMsExVhKg6lWUkHHHfvWZpHxSbXtVW20rw9fzQpctb3Fx8oWIhiuevPrxXPWFhYT/Ei0z4nv9fvrO2lfdiMxxKQAQxVR14/Kui+DiKvhO8YAAtqNzk+v71qabbsDSSueg0UUVZmFFFFABRRRQAUUUUAFFFFAHO6t/yUfwP/ANf11/6RTV6NXnOrf8lH8D/9f11/6RTV6NXNU+I6afwhRRRWZoFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAHjnwx/wCQt49/7Gu9/wDQhXf1wHwx/wCQt49/7Gu9/wDQhXf11Q+FHLP4mFFFFWQFFFFABRRRQAUUUUAFQ3VrBe2sltdxLNDKpV43GQwPUGpqKAILOyttPtI7WxhSCCMYSNBgKKji0uxge5eG1jRro5nIX/WHGOfWrdFIZiWvg7w7Y3wvLTR7SK5ByJFj5zWgdLsjey3htY/tE0flSS7fmZPQ+1Zet65Nba5pmj6cFa6vHLyEjPlxLjcfr6fSsO68TeItS1zWLXw8ljDBo2BMbvdulbBOBjoPlPNLRD1Zvt4M8OPJFI+jWhaH/Vnyx8taNxplldtbtc2schtm3wll/wBWfUelUPCXiAeJ/DNrqvk+S0wIdOwZSVOPbIOKItVeHXNTW+vbJbK2hjcKJAJIslsl/QcDH0NPQWpY1Xw9pGubP7X0+C72fd81M4qxp+mWWlWottNto7aFeiRrgVRh8V6DcakLCDV7OS6bpGsykk+g56+1Ov8AxPommSPHf6pawSRsFeN5QGU4zyOvQg0aBqW9R0yy1a0NrqdtHcwE5Mci5BNZI8BeFVYEaDZAjkfuq1f7VsPs0FwLyAw3DBYpBINshPQA9zT7jULO1mWK5uYopGUuqO4BIHJOPQUaBqUdQ8K6FqrRNqOl21w0KhYzImSoHQCnnR7LTt95pmmwC8jhMce0bSV/u59Kx/C/j3TvEGn393Lc2tvHZzujEzDAQHAYnPGa2NN8Q6TrfmJpOo29zIg5WOQEj3x6UaD1OY8I6NrN/wCLLzxV4psIrC6eIW9tapJ5nlrxklsDk4/Wuv1LSrDWLU2+qWkV1Cedkq5FZfhrXZ9RuNR0/UVVL/T5ykgXgMh5Rh9VxXQULYHuZ2l6BpOiwvFpWnwWiSffESY3VYstPtNNhMVhbxwRsxcqgwCxOSfzNWaKBBRRRTEFFFFABRRRQAUUUUAFFFFAHO6t/wAlH8D/APX9df8ApFNXo1ec6t/yUfwP/wBf11/6RTV6NXNU+I6afwhRRRWZoFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFAHjnwx/5C3j3/sa73/0IV39cB8Mf+Qt49/7Gu9/9CFd/XVD4Ucs/iYUUUVZAUUUUAFFFFABRRRQAUUUUAFFFFAHCRkn45T+f20iPyf++5M1gQaFZeLfEfiq61nUJ7Ga2ma2WG3kEYEYzhm9Scda7fXdEnl8RaXrmmgG5tWMUyk48yFuv4jnH1qTV/BPh7Xb1bzVNLgnuAMGRkGWHofWosXczPhbezXngmJZVXZbTS28UiLgSojsqt+IArkvEQzrfxGB76Pb/wDoUtes2trBZWsdtaRLDDGoVI0GAo9BVObQNLuJb2Sayhd76MRXLFf9agzgH1HJ/OnbQL6nnHiHR7Cw+HXhqezto4p4rm2KyquHyeDk9TnNXtBg0eb4u+Mf7QSCS5BhwswBwnkpyAfxrvbjRtPu7GGzuLSOS3hZWjjZeFK9MfSuRT4eWep+NNf1HxBYW91b3csbWxcZIAiRT+oNKwXOKjlWDw3bzK23S4fFI+zOT8qx7e3tnNdhr93bXPxi8OWySJKy20xdQc4BHeuvn8PaTcaKNImsIG08LtFuUGwD6VV0rwZ4f0WaObTNLt4JYslZFQbhn3osw5keYWFjpt18O9XtZryHTXbWGCzGMEBhL8gYcfLntW1oF1daX4906y8QaVpr3l1buttf6aSg2ArkOmT6jmu1XwZ4eWS+caTbZ1AYuh5YxL9aNI8IaF4emkuNH0yCCdlwXRQGI9M+lHKw5kYmm5Hxp1gR/cOmxmTH9/KY/Su3rnPC+i3Nld6lquqBft2ozlmAOdka8Iuf93FdHVIlhRRRTEFFFFABRRRQAUUUUAFFFFABRRRQBzurf8lH8D/9f11/6RTV6NXnOrf8lH8D/wDX9df+kU1ejVzVPiOmn8IUUUVmaBRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQB458Mf+Qt49/7Gu9/9CFd/WIvwks7fU9TvdM8T+ItOOp3kl7cQ2s8ATzZGyxAaInHbr2qX/hWMn/Q8+K//AAIt/wD4xW0aiSsYyptu5rUVk/8ACsZP+h58V/8AgRb/APxij/hWMn/Q8+K//Ai3/wDjFV7VC9kzWorJ/wCFYyf9Dz4r/wDAi3/+MUf8Kxk/6HnxX/4EW/8A8Yo9qg9kzWorJ/4VjJ/0PPiv/wACLf8A+MUf8Kxk/wCh58V/+BFv/wDGKPaoPZM1qKyf+FYyf9Dz4r/8CLf/AOMUf8Kxk/6HnxX/AOBFv/8AGKPaoPZM1qKyf+FYyf8AQ8+K/wDwIt//AIxR/wAKxk/6HnxX/wCBFv8A/GKPaoPZM1qKyf8AhWMn/Q8+K/8AwIt//jFH/CsZP+h58V/+BFv/APGKPaoPZM1qKyf+FYyf9Dz4r/8AAi3/APjFH/CsZP8AoefFf/gRb/8Axij2qD2TNaisn/hWMn/Q8+K//Ai3/wDjFH/CsZP+h58V/wDgRb//ABij2qD2TNaisn/hWMn/AEPPiv8A8CLf/wCMUf8ACsZP+h58V/8AgRb/APxij2qD2TNaisn/AIVjJ/0PPiv/AMCLf/4xR/wrGT/oefFf/gRb/wDxij2qD2TNaisn/hWMn/Q8+K//AAIt/wD4xR/wrGT/AKHnxX/4EW//AMYo9qg9kzWorJ/4VjJ/0PPiv/wIt/8A4xR/wrGT/oefFf8A4EW//wAYo9qg9kzWorJ/4VjJ/wBDz4r/APAi3/8AjFH/AArGT/oefFf/AIEW/wD8Yo9qg9kzWorJ/wCFYyf9Dz4r/wDAi3/+MUf8Kxk/6HnxX/4EW/8A8Yo9qg9kzWorJ/4VjJ/0PPiv/wACLf8A+MUf8Kxk/wCh58V/+BFv/wDGKPaoPZM1qKyf+FYyf9Dz4r/8CLf/AOMUf8Kxk/6HnxX/AOBFv/8AGKPaoPZM1qKyf+FYyf8AQ8+K/wDwIt//AIxR/wAKxk/6HnxX/wCBFv8A/GKPaoPZM1qKyf8AhWMn/Q8+K/8AwIt//jFH/CsZP+h58V/+BFv/APGKPaoPZMp6t/yUfwP/ANf11/6RTV6NXGaZ8NoNP8RafrFz4k17VJtOZ3givpoWjDPG0ZJCxKfuue9dnWMnd3NYqysFFFFSUFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAFFFFABRRRQAUUUUAf/Z\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"C:\\\\Users\\\\ShahinN\\\\Desktop\\\\network structure.JPG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ShahinN\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "model.compile(loss= 'binary_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ShahinN\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/150\n",
      "768/768 [==============================] - 1s 1ms/step - loss: 0.6828 - accuracy: 0.6276\n",
      "Epoch 2/150\n",
      "768/768 [==============================] - 0s 272us/step - loss: 0.6611 - accuracy: 0.6510\n",
      "Epoch 3/150\n",
      "768/768 [==============================] - 0s 259us/step - loss: 0.6464 - accuracy: 0.6536\n",
      "Epoch 4/150\n",
      "768/768 [==============================] - 0s 277us/step - loss: 0.6352 - accuracy: 0.6562\n",
      "Epoch 5/150\n",
      "768/768 [==============================] - 0s 275us/step - loss: 0.6175 - accuracy: 0.6576\n",
      "Epoch 6/150\n",
      "768/768 [==============================] - 0s 264us/step - loss: 0.6126 - accuracy: 0.6706\n",
      "Epoch 7/150\n",
      "768/768 [==============================] - 0s 255us/step - loss: 0.6002 - accuracy: 0.6823\n",
      "Epoch 8/150\n",
      "768/768 [==============================] - 0s 218us/step - loss: 0.5945 - accuracy: 0.6927\n",
      "Epoch 9/150\n",
      "768/768 [==============================] - 0s 270us/step - loss: 0.5874 - accuracy: 0.6979\n",
      "Epoch 10/150\n",
      "768/768 [==============================] - 0s 260us/step - loss: 0.5918 - accuracy: 0.6823\n",
      "Epoch 11/150\n",
      "768/768 [==============================] - 0s 277us/step - loss: 0.5838 - accuracy: 0.7044\n",
      "Epoch 12/150\n",
      "768/768 [==============================] - 0s 308us/step - loss: 0.5824 - accuracy: 0.6888\n",
      "Epoch 13/150\n",
      "768/768 [==============================] - 0s 318us/step - loss: 0.5804 - accuracy: 0.7018\n",
      "Epoch 14/150\n",
      "768/768 [==============================] - 0s 281us/step - loss: 0.5831 - accuracy: 0.6888\n",
      "Epoch 15/150\n",
      "768/768 [==============================] - 0s 268us/step - loss: 0.5783 - accuracy: 0.6992\n",
      "Epoch 16/150\n",
      "768/768 [==============================] - 0s 262us/step - loss: 0.5699 - accuracy: 0.6992\n",
      "Epoch 17/150\n",
      "768/768 [==============================] - 0s 263us/step - loss: 0.5666 - accuracy: 0.7122\n",
      "Epoch 18/150\n",
      "768/768 [==============================] - 0s 268us/step - loss: 0.5663 - accuracy: 0.7227\n",
      "Epoch 19/150\n",
      "768/768 [==============================] - 0s 262us/step - loss: 0.5653 - accuracy: 0.7227\n",
      "Epoch 20/150\n",
      "768/768 [==============================] - 0s 266us/step - loss: 0.5648 - accuracy: 0.7214\n",
      "Epoch 21/150\n",
      "768/768 [==============================] - 0s 320us/step - loss: 0.5597 - accuracy: 0.7227\n",
      "Epoch 22/150\n",
      "768/768 [==============================] - 0s 285us/step - loss: 0.5568 - accuracy: 0.7135\n",
      "Epoch 23/150\n",
      "768/768 [==============================] - 0s 284us/step - loss: 0.5549 - accuracy: 0.7318\n",
      "Epoch 24/150\n",
      "768/768 [==============================] - 0s 288us/step - loss: 0.5514 - accuracy: 0.7396\n",
      "Epoch 25/150\n",
      "768/768 [==============================] - 0s 272us/step - loss: 0.5536 - accuracy: 0.7331\n",
      "Epoch 26/150\n",
      "768/768 [==============================] - 0s 258us/step - loss: 0.5480 - accuracy: 0.7409\n",
      "Epoch 27/150\n",
      "768/768 [==============================] - 0s 267us/step - loss: 0.5533 - accuracy: 0.7292\n",
      "Epoch 28/150\n",
      "768/768 [==============================] - 0s 262us/step - loss: 0.5460 - accuracy: 0.7305\n",
      "Epoch 29/150\n",
      "768/768 [==============================] - 0s 286us/step - loss: 0.5442 - accuracy: 0.7383\n",
      "Epoch 30/150\n",
      "768/768 [==============================] - 0s 268us/step - loss: 0.5417 - accuracy: 0.7240\n",
      "Epoch 31/150\n",
      "768/768 [==============================] - 0s 276us/step - loss: 0.5406 - accuracy: 0.7409\n",
      "Epoch 32/150\n",
      "768/768 [==============================] - 0s 267us/step - loss: 0.5395 - accuracy: 0.7253\n",
      "Epoch 33/150\n",
      "768/768 [==============================] - 0s 262us/step - loss: 0.5404 - accuracy: 0.7422\n",
      "Epoch 34/150\n",
      "768/768 [==============================] - 0s 263us/step - loss: 0.5383 - accuracy: 0.7318\n",
      "Epoch 35/150\n",
      "768/768 [==============================] - 0s 293us/step - loss: 0.5357 - accuracy: 0.7344\n",
      "Epoch 36/150\n",
      "768/768 [==============================] - 0s 253us/step - loss: 0.5338 - accuracy: 0.7539\n",
      "Epoch 37/150\n",
      "768/768 [==============================] - 0s 257us/step - loss: 0.5329 - accuracy: 0.7370\n",
      "Epoch 38/150\n",
      "768/768 [==============================] - 0s 260us/step - loss: 0.5316 - accuracy: 0.7344\n",
      "Epoch 39/150\n",
      "768/768 [==============================] - 0s 253us/step - loss: 0.5326 - accuracy: 0.7461\n",
      "Epoch 40/150\n",
      "768/768 [==============================] - 0s 253us/step - loss: 0.5257 - accuracy: 0.7539\n",
      "Epoch 41/150\n",
      "768/768 [==============================] - 0s 247us/step - loss: 0.5316 - accuracy: 0.7435\n",
      "Epoch 42/150\n",
      "768/768 [==============================] - 0s 259us/step - loss: 0.5244 - accuracy: 0.7435\n",
      "Epoch 43/150\n",
      "768/768 [==============================] - 0s 268us/step - loss: 0.5196 - accuracy: 0.7565\n",
      "Epoch 44/150\n",
      "768/768 [==============================] - 0s 264us/step - loss: 0.5169 - accuracy: 0.7578\n",
      "Epoch 45/150\n",
      "768/768 [==============================] - 0s 305us/step - loss: 0.5117 - accuracy: 0.7565\n",
      "Epoch 46/150\n",
      "768/768 [==============================] - 0s 293us/step - loss: 0.5258 - accuracy: 0.7357\n",
      "Epoch 47/150\n",
      "768/768 [==============================] - 0s 294us/step - loss: 0.5142 - accuracy: 0.7565\n",
      "Epoch 48/150\n",
      "768/768 [==============================] - 0s 260us/step - loss: 0.5139 - accuracy: 0.75650s - loss: 0.5152 - accuracy: \n",
      "Epoch 49/150\n",
      "768/768 [==============================] - 0s 254us/step - loss: 0.5095 - accuracy: 0.7617\n",
      "Epoch 50/150\n",
      "768/768 [==============================] - 0s 289us/step - loss: 0.5118 - accuracy: 0.7578\n",
      "Epoch 51/150\n",
      "768/768 [==============================] - 0s 253us/step - loss: 0.5042 - accuracy: 0.7604\n",
      "Epoch 52/150\n",
      "768/768 [==============================] - 0s 264us/step - loss: 0.5180 - accuracy: 0.7461\n",
      "Epoch 53/150\n",
      "768/768 [==============================] - 0s 310us/step - loss: 0.5072 - accuracy: 0.7643\n",
      "Epoch 54/150\n",
      "768/768 [==============================] - 0s 335us/step - loss: 0.5096 - accuracy: 0.7591\n",
      "Epoch 55/150\n",
      "768/768 [==============================] - 0s 272us/step - loss: 0.5100 - accuracy: 0.7630\n",
      "Epoch 56/150\n",
      "768/768 [==============================] - 0s 285us/step - loss: 0.5017 - accuracy: 0.7604\n",
      "Epoch 57/150\n",
      "768/768 [==============================] - 0s 285us/step - loss: 0.5096 - accuracy: 0.7734\n",
      "Epoch 58/150\n",
      "768/768 [==============================] - 0s 294us/step - loss: 0.5056 - accuracy: 0.7500\n",
      "Epoch 59/150\n",
      "768/768 [==============================] - 0s 328us/step - loss: 0.5018 - accuracy: 0.77210s - loss: 0.5191 - accuracy: \n",
      "Epoch 60/150\n",
      "768/768 [==============================] - 0s 362us/step - loss: 0.4990 - accuracy: 0.7565\n",
      "Epoch 61/150\n",
      "768/768 [==============================] - 0s 279us/step - loss: 0.4994 - accuracy: 0.7565\n",
      "Epoch 62/150\n",
      "768/768 [==============================] - 0s 284us/step - loss: 0.4928 - accuracy: 0.7695\n",
      "Epoch 63/150\n",
      "768/768 [==============================] - 0s 293us/step - loss: 0.4976 - accuracy: 0.7682\n",
      "Epoch 64/150\n",
      "768/768 [==============================] - 0s 350us/step - loss: 0.4979 - accuracy: 0.7643\n",
      "Epoch 65/150\n",
      "768/768 [==============================] - 0s 294us/step - loss: 0.5003 - accuracy: 0.7591\n",
      "Epoch 66/150\n",
      "768/768 [==============================] - 0s 286us/step - loss: 0.4956 - accuracy: 0.7617\n",
      "Epoch 67/150\n",
      "768/768 [==============================] - 0s 273us/step - loss: 0.4939 - accuracy: 0.7682\n",
      "Epoch 68/150\n",
      "768/768 [==============================] - 0s 285us/step - loss: 0.5010 - accuracy: 0.7682\n",
      "Epoch 69/150\n",
      "768/768 [==============================] - 0s 324us/step - loss: 0.4972 - accuracy: 0.7526\n",
      "Epoch 70/150\n",
      "768/768 [==============================] - 0s 280us/step - loss: 0.4959 - accuracy: 0.7669\n",
      "Epoch 71/150\n",
      "768/768 [==============================] - 0s 279us/step - loss: 0.4871 - accuracy: 0.7708\n",
      "Epoch 72/150\n",
      "768/768 [==============================] - 0s 267us/step - loss: 0.4878 - accuracy: 0.7747\n",
      "Epoch 73/150\n",
      "768/768 [==============================] - 0s 273us/step - loss: 0.4890 - accuracy: 0.7708\n",
      "Epoch 74/150\n",
      "768/768 [==============================] - 0s 294us/step - loss: 0.4862 - accuracy: 0.7747\n",
      "Epoch 75/150\n",
      "768/768 [==============================] - 0s 275us/step - loss: 0.4855 - accuracy: 0.7826\n",
      "Epoch 76/150\n",
      "768/768 [==============================] - 0s 264us/step - loss: 0.4903 - accuracy: 0.7630\n",
      "Epoch 77/150\n",
      "768/768 [==============================] - 0s 299us/step - loss: 0.4852 - accuracy: 0.7630\n",
      "Epoch 78/150\n",
      "768/768 [==============================] - 0s 302us/step - loss: 0.4876 - accuracy: 0.7695\n",
      "Epoch 79/150\n",
      "768/768 [==============================] - 0s 314us/step - loss: 0.4831 - accuracy: 0.7708\n",
      "Epoch 80/150\n",
      "768/768 [==============================] - 0s 298us/step - loss: 0.4837 - accuracy: 0.7865\n",
      "Epoch 81/150\n",
      "768/768 [==============================] - 0s 294us/step - loss: 0.4837 - accuracy: 0.7760\n",
      "Epoch 82/150\n",
      "768/768 [==============================] - 0s 270us/step - loss: 0.4841 - accuracy: 0.7747\n",
      "Epoch 83/150\n",
      "768/768 [==============================] - 0s 290us/step - loss: 0.4755 - accuracy: 0.7760\n",
      "Epoch 84/150\n",
      "768/768 [==============================] - 0s 276us/step - loss: 0.4843 - accuracy: 0.7773\n",
      "Epoch 85/150\n",
      "768/768 [==============================] - 0s 276us/step - loss: 0.4769 - accuracy: 0.7799\n",
      "Epoch 86/150\n",
      "768/768 [==============================] - 0s 282us/step - loss: 0.4752 - accuracy: 0.7839\n",
      "Epoch 87/150\n",
      "768/768 [==============================] - 0s 264us/step - loss: 0.4761 - accuracy: 0.7747\n",
      "Epoch 88/150\n",
      "768/768 [==============================] - 0s 270us/step - loss: 0.4795 - accuracy: 0.7812\n",
      "Epoch 89/150\n",
      "768/768 [==============================] - 0s 264us/step - loss: 0.4753 - accuracy: 0.7839\n",
      "Epoch 90/150\n",
      "768/768 [==============================] - 0s 277us/step - loss: 0.4758 - accuracy: 0.7812\n",
      "Epoch 91/150\n",
      "768/768 [==============================] - 0s 255us/step - loss: 0.4663 - accuracy: 0.7852\n",
      "Epoch 92/150\n",
      "768/768 [==============================] - 0s 260us/step - loss: 0.4729 - accuracy: 0.7747\n",
      "Epoch 93/150\n",
      "768/768 [==============================] - 0s 276us/step - loss: 0.4765 - accuracy: 0.7878\n",
      "Epoch 94/150\n",
      "768/768 [==============================] - 0s 286us/step - loss: 0.4697 - accuracy: 0.7786\n",
      "Epoch 95/150\n",
      "768/768 [==============================] - 0s 286us/step - loss: 0.4804 - accuracy: 0.7565\n",
      "Epoch 96/150\n",
      "768/768 [==============================] - 0s 297us/step - loss: 0.4715 - accuracy: 0.7747\n",
      "Epoch 97/150\n",
      "768/768 [==============================] - 0s 263us/step - loss: 0.4721 - accuracy: 0.7826\n",
      "Epoch 98/150\n",
      "768/768 [==============================] - 0s 270us/step - loss: 0.4686 - accuracy: 0.7917\n",
      "Epoch 99/150\n",
      "768/768 [==============================] - 0s 285us/step - loss: 0.4659 - accuracy: 0.7891\n",
      "Epoch 100/150\n",
      "768/768 [==============================] - 0s 257us/step - loss: 0.4648 - accuracy: 0.7773\n",
      "Epoch 101/150\n",
      "768/768 [==============================] - 0s 260us/step - loss: 0.4654 - accuracy: 0.7865\n",
      "Epoch 102/150\n",
      "768/768 [==============================] - 0s 254us/step - loss: 0.4674 - accuracy: 0.7773\n",
      "Epoch 103/150\n",
      "768/768 [==============================] - 0s 259us/step - loss: 0.4610 - accuracy: 0.7943\n",
      "Epoch 104/150\n",
      "768/768 [==============================] - 0s 270us/step - loss: 0.4710 - accuracy: 0.7630\n",
      "Epoch 105/150\n",
      "768/768 [==============================] - 0s 253us/step - loss: 0.4733 - accuracy: 0.7747\n",
      "Epoch 106/150\n",
      "768/768 [==============================] - 0s 259us/step - loss: 0.4626 - accuracy: 0.7878\n",
      "Epoch 107/150\n",
      "768/768 [==============================] - 0s 257us/step - loss: 0.4663 - accuracy: 0.7995\n",
      "Epoch 108/150\n",
      "768/768 [==============================] - 0s 256us/step - loss: 0.4614 - accuracy: 0.7865\n",
      "Epoch 109/150\n",
      "768/768 [==============================] - 0s 259us/step - loss: 0.4723 - accuracy: 0.7773\n",
      "Epoch 110/150\n",
      "768/768 [==============================] - 0s 258us/step - loss: 0.4632 - accuracy: 0.7852\n",
      "Epoch 111/150\n",
      "768/768 [==============================] - 0s 247us/step - loss: 0.4649 - accuracy: 0.7760\n",
      "Epoch 112/150\n",
      "768/768 [==============================] - 0s 260us/step - loss: 0.4678 - accuracy: 0.7760\n",
      "Epoch 113/150\n",
      "768/768 [==============================] - 0s 250us/step - loss: 0.4528 - accuracy: 0.7878\n",
      "Epoch 114/150\n",
      "768/768 [==============================] - 0s 254us/step - loss: 0.4543 - accuracy: 0.7917\n",
      "Epoch 115/150\n",
      "768/768 [==============================] - 0s 253us/step - loss: 0.4619 - accuracy: 0.7826\n",
      "Epoch 116/150\n",
      "768/768 [==============================] - 0s 257us/step - loss: 0.4679 - accuracy: 0.7839\n",
      "Epoch 117/150\n",
      "768/768 [==============================] - 0s 263us/step - loss: 0.4576 - accuracy: 0.7878\n",
      "Epoch 118/150\n",
      "768/768 [==============================] - 0s 249us/step - loss: 0.4615 - accuracy: 0.7891\n",
      "Epoch 119/150\n",
      "768/768 [==============================] - 0s 253us/step - loss: 0.4657 - accuracy: 0.7630\n",
      "Epoch 120/150\n",
      "768/768 [==============================] - 0s 258us/step - loss: 0.4601 - accuracy: 0.7956\n",
      "Epoch 121/150\n",
      "768/768 [==============================] - 0s 253us/step - loss: 0.4492 - accuracy: 0.7969\n",
      "Epoch 122/150\n",
      "768/768 [==============================] - 0s 251us/step - loss: 0.4496 - accuracy: 0.7995\n",
      "Epoch 123/150\n",
      "768/768 [==============================] - 0s 281us/step - loss: 0.4518 - accuracy: 0.7982\n",
      "Epoch 124/150\n",
      "768/768 [==============================] - 0s 257us/step - loss: 0.4616 - accuracy: 0.7826\n",
      "Epoch 125/150\n",
      "768/768 [==============================] - 0s 254us/step - loss: 0.4532 - accuracy: 0.7969\n",
      "Epoch 126/150\n",
      "768/768 [==============================] - 0s 255us/step - loss: 0.4562 - accuracy: 0.7852\n",
      "Epoch 127/150\n",
      "768/768 [==============================] - 0s 259us/step - loss: 0.4559 - accuracy: 0.7917\n",
      "Epoch 128/150\n",
      "768/768 [==============================] - 0s 251us/step - loss: 0.4523 - accuracy: 0.7917\n",
      "Epoch 129/150\n",
      "768/768 [==============================] - 0s 251us/step - loss: 0.4512 - accuracy: 0.7982\n",
      "Epoch 130/150\n",
      "768/768 [==============================] - 0s 260us/step - loss: 0.4538 - accuracy: 0.7839\n",
      "Epoch 131/150\n",
      "768/768 [==============================] - 0s 260us/step - loss: 0.4505 - accuracy: 0.8021\n",
      "Epoch 132/150\n",
      "768/768 [==============================] - 0s 247us/step - loss: 0.4466 - accuracy: 0.7943\n",
      "Epoch 133/150\n",
      "768/768 [==============================] - 0s 267us/step - loss: 0.4564 - accuracy: 0.7852\n",
      "Epoch 134/150\n",
      "768/768 [==============================] - 0s 250us/step - loss: 0.4439 - accuracy: 0.7982\n",
      "Epoch 135/150\n",
      "768/768 [==============================] - 0s 265us/step - loss: 0.4462 - accuracy: 0.7891\n",
      "Epoch 136/150\n",
      "768/768 [==============================] - 0s 257us/step - loss: 0.4489 - accuracy: 0.7839\n",
      "Epoch 137/150\n",
      "768/768 [==============================] - 0s 284us/step - loss: 0.4441 - accuracy: 0.7982\n",
      "Epoch 138/150\n",
      "768/768 [==============================] - 0s 266us/step - loss: 0.4572 - accuracy: 0.7812\n",
      "Epoch 139/150\n",
      "768/768 [==============================] - 0s 250us/step - loss: 0.4522 - accuracy: 0.7917\n",
      "Epoch 140/150\n",
      "768/768 [==============================] - 0s 294us/step - loss: 0.4466 - accuracy: 0.7891\n",
      "Epoch 141/150\n",
      "768/768 [==============================] - 0s 298us/step - loss: 0.4421 - accuracy: 0.7852\n",
      "Epoch 142/150\n",
      "768/768 [==============================] - 0s 255us/step - loss: 0.4548 - accuracy: 0.7878\n",
      "Epoch 143/150\n",
      "768/768 [==============================] - 0s 269us/step - loss: 0.4454 - accuracy: 0.7969\n",
      "Epoch 144/150\n",
      "768/768 [==============================] - 0s 251us/step - loss: 0.4466 - accuracy: 0.7891\n",
      "Epoch 145/150\n",
      "768/768 [==============================] - 0s 254us/step - loss: 0.4401 - accuracy: 0.8047\n",
      "Epoch 146/150\n",
      "768/768 [==============================] - 0s 254us/step - loss: 0.4608 - accuracy: 0.7878\n",
      "Epoch 147/150\n",
      "768/768 [==============================] - 0s 260us/step - loss: 0.4560 - accuracy: 0.7852\n",
      "Epoch 148/150\n",
      "768/768 [==============================] - 0s 259us/step - loss: 0.4408 - accuracy: 0.7904\n",
      "Epoch 149/150\n",
      "768/768 [==============================] - 0s 257us/step - loss: 0.4417 - accuracy: 0.8034\n",
      "Epoch 150/150\n",
      "768/768 [==============================] - 0s 255us/step - loss: 0.4463 - accuracy: 0.7891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x15ad61bfe48>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit Model\n",
    "model.fit(X, Y, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 0s 236us/step\n",
      "accuracy: 78.91%\n"
     ]
    }
   ],
   "source": [
    "# ارزیابی مدل\n",
    "scores = model.evaluate(X, Y)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ذخیره کردن مدل شبکه عصبی با فرمت Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_json = model.to_json()\n",
    "\n",
    "with open(\"C:\\\\Users\\\\ShahinN\\\\Desktop\\\\model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# ذخیره سازی وزن های مدل\n",
    "model.save_weights(\"C:\\\\Users\\\\ShahinN\\\\Desktop\\\\model.h5\")\n",
    "\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "# بارگذاری فایل json و ساخت مدل\n",
    "json_file = open(\"C:\\\\Users\\\\ShahinN\\\\Desktop\\\\model.json\" , 'r' )\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "# بازگذاری وزن ها در مدل جدید\n",
    "loaded_model.load_weights(\"C:\\\\Users\\\\ShahinN\\\\Desktop\\\\model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 78.91%\n"
     ]
    }
   ],
   "source": [
    "# ارزیابی دقت پیش بینی مدل\n",
    "loaded_model.compile(loss= 'binary_crossentropy' , optimizer= 'rmsprop' , metrics=[ 'accuracy' ])\n",
    "score = loaded_model.evaluate(X, Y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# حفظ بهترین مدل ها هنگام آموزش با استفاده از check point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{val_accuracy:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor= 'val_accuracy' , verbose=1, save_best_only=True,\n",
    "mode= 'max' )\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.80709, saving model to weights-improvement-01-0.81.hdf5\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.80709 to 0.81102, saving model to weights-improvement-02-0.81.hdf5\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.81102\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.81102\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.81102 to 0.81496, saving model to weights-improvement-05-0.81.hdf5\n",
      "\n",
      "Epoch 00006: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00020: val_accuracy improved from 0.81496 to 0.81890, saving model to weights-improvement-20-0.82.hdf5\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00028: val_accuracy improved from 0.81890 to 0.82283, saving model to weights-improvement-28-0.82.hdf5\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00074: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00084: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00086: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00089: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00090: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00092: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00093: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00094: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00095: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00096: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00097: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00098: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00099: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00100: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00101: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00102: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00103: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00104: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00105: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00106: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00107: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00108: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00109: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00110: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00111: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00112: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00113: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00114: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00115: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00116: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00117: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00118: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00119: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00120: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00121: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00122: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00123: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00124: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00125: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00126: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00127: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00128: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00129: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00130: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00131: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00132: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00133: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00134: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00135: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00136: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00137: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00138: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00139: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00140: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00141: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00142: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00143: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00144: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00145: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00146: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00147: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00148: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00149: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00150: val_accuracy did not improve from 0.82283\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x15ade0f0550>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10,\n",
    "callbacks=callbacks_list, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 0s 102us/step\n"
     ]
    }
   ],
   "source": [
    "# ارزیابی مدل\n",
    "scores = model.evaluate(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.77083134651184\n"
     ]
    }
   ],
   "source": [
    "print(scores[1]*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint بهترین مدل فقط"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.79134, saving model to weights.best.hdf5\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.79134 to 0.79921, saving model to weights.best.hdf5\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.79921 to 0.80315, saving model to weights.best.hdf5\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.80315\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.80315 to 0.80709, saving model to weights.best.hdf5\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.80709 to 0.81496, saving model to weights.best.hdf5\n",
      "\n",
      "Epoch 00007: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00008: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00009: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00010: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00011: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00012: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00013: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00014: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00015: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00016: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00018: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00021: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00024: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.81496\n",
      "\n",
      "Epoch 00029: val_accuracy improved from 0.81496 to 0.81890, saving model to weights.best.hdf5\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00034: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00035: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00051: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00066: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00067: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00068: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00069: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00070: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00071: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00072: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00073: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00074: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00075: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00076: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00077: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00078: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00079: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00080: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00081: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00082: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00083: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00084: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00085: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00086: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00087: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00088: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00089: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00090: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00091: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00092: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00093: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00094: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00095: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00096: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00097: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00098: val_accuracy did not improve from 0.81890\n",
      "\n",
      "Epoch 00099: val_accuracy improved from 0.81890 to 0.82283, saving model to weights.best.hdf5\n",
      "\n",
      "Epoch 00100: val_accuracy did not improve from 0.82283\n",
      "\n",
      "Epoch 00101: val_accuracy improved from 0.82283 to 0.82677, saving model to weights.best.hdf5\n",
      "\n",
      "Epoch 00102: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00103: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00104: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00105: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00106: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00107: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00108: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00109: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00110: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00111: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00112: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00113: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00114: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00115: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00116: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00117: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00118: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00119: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00120: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00121: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00122: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00123: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00124: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00125: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00126: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00127: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00128: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00129: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00130: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00131: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00132: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00133: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00134: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00135: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00136: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00137: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00138: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00139: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00140: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00141: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00142: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00143: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00144: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00145: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00146: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00147: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00148: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00149: val_accuracy did not improve from 0.82677\n",
      "\n",
      "Epoch 00150: val_accuracy did not improve from 0.82677\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x15adf14e208>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checkpoint\n",
    "filepath=\"weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor= 'val_accuracy' , verbose=1, save_best_only=True,\n",
    "            mode= 'max' )\n",
    "callbacks_list = [checkpoint]\n",
    "# Fit the model\n",
    "model.fit(X, Y, validation_split=0.33, epochs=150, batch_size=10,\n",
    "            callbacks=callbacks_list, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768/768 [==============================] - 0s 122us/step\n",
      "81.11979365348816\n"
     ]
    }
   ],
   "source": [
    "# ارزیابی مدل\n",
    "scores = model.evaluate(X, Y)\n",
    "print(scores[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
