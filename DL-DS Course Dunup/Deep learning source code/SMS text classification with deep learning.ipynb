{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Target', 'Email'], dtype='object')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#خواندن دیتا\n",
    "import pandas as pd\n",
    "Email_Data = pd.read_csv(\"C:\\\\Users\\\\ShahinN\\\\Desktop\\\\SMSSpamCollection.txt\", sep= '\\t', header=None, names=[\"Target\", \"Email\"])\n",
    "\n",
    "Email_Data.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target                                              Email\n",
       "0    ham  Go until jurong point, crazy.. Available only ...\n",
       "1    ham                      Ok lar... Joking wif u oni...\n",
       "2   spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3    ham  U dun say so early hor... U c already then say...\n",
       "4    ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Email_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import Word\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import sklearn.feature_extraction.text as text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# پیش پردازش متن دیتاست (پیامک ها"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "      <th>Email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>go jurong point, crazy.. avail bugi n great wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>ok lar... joke wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>free entri 2 wkli comp win fa cup final tkt 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>u dun say earli hor... u c alreadi say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>nah think goe usf, live around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Target                                              Email\n",
       "0    ham  go jurong point, crazy.. avail bugi n great wo...\n",
       "1    ham                        ok lar... joke wif u oni...\n",
       "2   spam  free entri 2 wkli comp win fa cup final tkt 21...\n",
       "3    ham          u dun say earli hor... u c alreadi say...\n",
       "4    ham              nah think goe usf, live around though"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lowercase\n",
    "Email_Data['Email'] =  Email_Data['Email'].apply(lambda x:  \" \".join(x.lower() for x in x.split()))\n",
    "# stopword filtering\n",
    "stop = stopwords.words('english')\n",
    "Email_Data['Email'] =  Email_Data['Email'].apply(lambda x: \" \".join (x for x in x.split() if x not in stop))\n",
    "#stemming\n",
    "st = PorterStemmer()\n",
    "Email_Data['Email'] =  Email_Data['Email'].apply(lambda x: \" \".join ([st.stem(word) for word in x.split()]))\n",
    "#lemmatize\n",
    "Email_Data['Email'] =  Email_Data['Email'].apply(lambda x: \" \".join ([Word(word).lemmatize() for word in x.split()]))\n",
    "Email_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAELCAYAAAA1AlaNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAPyElEQVR4nO3df8ydZX3H8fdHCujmlCIPjLRoMfYPUZniEyBxyQxspcKykkxczTIb16T/MOM2E8VFQwRJcH+Ic1G3ZhArU5A4TVFRbFBclg2lFccPkfQZIHRFW1JAnZFZ/O6Pc1UfyvMTnuec7lzvV3Jy7vt7X/c51x1OP+fiOtc5T6oKSVIfnjfqDkiShsfQl6SOGPqS1BFDX5I6YuhLUkcMfUnqyIJCP8mDSe5K8t0kO1vt+CQ7kuxu9ytbPUk+mmQqyZ1Jzpj2OJta+91JNi3PJUmSZpOFrNNP8iAwWVWPTqv9LXCgqq5Mcgmwsqrek+R84B3A+cBZwN9V1VlJjgd2ApNAAbuA11fVY7M97wknnFBr1qx51hcnST3atWvXo1U1MdOxFc/hcTcAb2zb24Bbgfe0+qdq8G5yW5Ljkpzc2u6oqgMASXYA64HrZnuCNWvWsHPnzufQRUnqT5IfzHZsoXP6BXwtya4kW1rtpKp6BKDdn9jqq4CHp527p9Vmq0uShmShI/03VNXeJCcCO5J8f462maFWc9SffvLgTWULwEtf+tIFdk+StBALGulX1d52vw/4AnAm8KM2bUO739ea7wFOmXb6amDvHPXDn2trVU1W1eTExIxTUpKkZ2ne0E/ym0l+69A2sA64G7gROLQCZxOwvW3fCLytreI5G3iiTf/cDKxLsrKt9FnXapKkIVnI9M5JwBeSHGr/mar6apLbgRuSbAYeAi5q7W9isHJnCvgZ8HaAqjqQ5HLg9tbuskMf6kqShmNBSzZHZXJysly9I0mLk2RXVU3OdMxv5EpSRwx9SerIc/lylpo1l3x51F0YKw9eecGouyCNLUf6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1ZMGhn+SoJHck+VLbPzXJt5LsTvLZJMe0+rFtf6odXzPtMd7b6vclOW+pL0aSNLfFjPTfCdw7bf9DwFVVtRZ4DNjc6puBx6rqFcBVrR1JTgM2Aq8C1gMfT3LUc+u+JGkxFhT6SVYDFwD/1PYDnAN8rjXZBlzYtje0fdrxc1v7DcD1VfVkVT0ATAFnLsVFSJIWZqEj/Y8A7wZ+2fZfAjxeVQfb/h5gVdteBTwM0I4/0dr/qj7DOZKkIZg39JP8IbCvqnZNL8/QtOY5Ntc5059vS5KdSXbu379/vu5JkhZhISP9NwB/lORB4HoG0zofAY5LsqK1WQ3sbdt7gFMA2vEXAwem12c451eqamtVTVbV5MTExKIvSJI0u3lDv6reW1Wrq2oNgw9iv15Vfwp8A3hza7YJ2N62b2z7tONfr6pq9Y1tdc+pwFrg20t2JZKkea2Yv8ms3gNcn+SDwB3A1a1+NXBtkikGI/yNAFV1T5IbgO8BB4GLq+qp5/D8kqRFWlToV9WtwK1t+35mWH1TVT8HLprl/CuAKxbbSUnS0vAbuZLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6Mm/oJ3l+km8n+c8k9yT5QKufmuRbSXYn+WySY1r92LY/1Y6vmfZY7231+5Kct1wXJUma2UJG+k8C51TV7wCvBdYnORv4EHBVVa0FHgM2t/abgceq6hXAVa0dSU4DNgKvAtYDH09y1FJejCRpbvOGfg38tO0e3W4FnAN8rtW3ARe27Q1tn3b83CRp9eur6smqegCYAs5ckquQJC3Igub0kxyV5LvAPmAH8F/A41V1sDXZA6xq26uAhwHa8SeAl0yvz3COJGkIFhT6VfVUVb0WWM1gdP7KmZq1+8xybLb60yTZkmRnkp379+9fSPckSQu0qNU7VfU4cCtwNnBckhXt0Gpgb9veA5wC0I6/GDgwvT7DOdOfY2tVTVbV5MTExGK6J0max0JW70wkOa5tvwD4feBe4BvAm1uzTcD2tn1j26cd/3pVVatvbKt7TgXWAt9eqguRJM1vxfxNOBnY1lbaPA+4oaq+lOR7wPVJPgjcAVzd2l8NXJtkisEIfyNAVd2T5Abge8BB4OKqemppL0eSNJd5Q7+q7gReN0P9fmZYfVNVPwcumuWxrgCuWHw3JUlLwW/kSlJHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SerIvKGf5JQk30hyb5J7kryz1Y9PsiPJ7na/stWT5KNJppLcmeSMaY+1qbXfnWTT8l2WJGkmCxnpHwTeVVWvBM4GLk5yGnAJcEtVrQVuafsAbwLWttsW4BMweJMALgXOAs4ELj30RiFJGo55Q7+qHqmq77TtnwD3AquADcC21mwbcGHb3gB8qgZuA45LcjJwHrCjqg5U1WPADmD9kl6NJGlOi5rTT7IGeB3wLeCkqnoEBm8MwImt2Srg4Wmn7Wm12eqSpCFZcOgneSHwL8BfVtWP52o6Q63mqB/+PFuS7Eyyc//+/QvtniRpARYU+kmOZhD4n66qz7fyj9q0De1+X6vvAU6ZdvpqYO8c9aepqq1VNVlVkxMTE4u5FknSPBayeifA1cC9VfXhaYduBA6twNkEbJ9Wf1tbxXM28ESb/rkZWJdkZfsAd12rSZKGZMUC2rwB+DPgriTfbbW/Aa4EbkiyGXgIuKgduwk4H5gCfga8HaCqDiS5HLi9tbusqg4syVVIkhZk3tCvqn9j5vl4gHNnaF/AxbM81jXANYvpoCRp6fiNXEnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdmTf0k1yTZF+Su6fVjk+yI8nudr+y1ZPko0mmktyZ5Ixp52xq7Xcn2bQ8lyNJmstCRvqfBNYfVrsEuKWq1gK3tH2ANwFr220L8AkYvEkAlwJnAWcClx56o5AkDc+8oV9V/wocOKy8AdjWtrcBF06rf6oGbgOOS3IycB6wo6oOVNVjwA6e+UYiSVpmz3ZO/6SqegSg3Z/Y6quAh6e129Nqs9UlSUO01B/kZoZazVF/5gMkW5LsTLJz//79S9o5Serdsw39H7VpG9r9vlbfA5wyrd1qYO8c9Weoqq1VNVlVkxMTE8+ye5KkmTzb0L8ROLQCZxOwfVr9bW0Vz9nAE23652ZgXZKV7QPcda0mSRqiFfM1SHId8EbghCR7GKzCuRK4Iclm4CHgotb8JuB8YAr4GfB2gKo6kORy4PbW7rKqOvzDYUnSMps39KvqrbMcOneGtgVcPMvjXANcs6jeSZKWlN/IlaSOGPqS1BFDX5I6YuhLUkcMfUnqyLyrdyT9/7bmki+Pugtj48ErLxh1F54zR/qS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqyNBDP8n6JPclmUpyybCfX5J6NtTQT3IU8DHgTcBpwFuTnDbMPkhSz4Y90j8TmKqq+6vqf4HrgQ1D7oMkdWvYob8KeHja/p5WkyQNwYohP19mqNXTGiRbgC1t96dJ7lv2XvXjBODRUXdiPvnQqHugEfC1ubReNtuBYYf+HuCUafurgb3TG1TVVmDrMDvViyQ7q2py1P2QDudrc3iGPb1zO7A2yalJjgE2AjcOuQ+S1K2hjvSr6mCSvwBuBo4Crqmqe4bZB0nq2bCnd6iqm4Cbhv28Apw205HL1+aQpKrmbyVJGgv+DIMkdcTQl6SOGPqS1JGhf5Cr4UtyOrCGaf+9q+rzI+uQxK9+i+sCnvna/PCo+tQDQ3/MJbkGOB24B/hlKxdg6GvUvgj8HLiLX782tcwM/fF3dlX5S6Y6Eq2uqtNH3YneOKc//v7Dn6/WEeorSdaNuhO9caQ//rYxCP4fAk8y+NG7coSlI8BtwBeSPA/4Bb9+bb5otN0ab345a8wlmQL+msPmTavqByPrlAQkuR+4ELirDKKhcaQ//h6qKn/UTkei3cDdBv5wGfrj7/tJPsNgpcSTh4ou2dQR4BHg1iRf4emvTZdsLiNDf/y9gME/qOkfmLlkU0eCB9rtmHbTEDinL0kdcaQ/5pI8H9gMvAp4/qF6Vf35yDolAUkmgHfzzNfmOSPrVAdcpz/+rgV+GzgP+CaDP1H5k5H2SBr4NPB94FTgA8CDDP66npaR0ztjLskdVfW6JHdW1elJjgZudjSlUUuyq6pef+i12WrfrKrfG3XfxpnTO+PvF+3+8SSvBn7I4AeupFE79Np8JMkFwF4G/yeqZWToj7+tSVYC72PwR+hfCLx/tF2SAPhgkhcD7wL+HngR8Fej7dL4c3pnzCU5FvhjBqP7o1u5quqykXVK0sj4Qe742w5sAA4CP223/xlpjyQgycuTfDHJo0n2Jdme5OWj7te4c6Q/5pLcXVWvHnU/pMMluQ34GHBdK20E3lFVZ42uV+PPkf74+/ckrxl1J6QZpKquraqD7fbPDL4trmXkSH9MJbmLwT+gFcBa4H78aWUdQZJcCTwOXM/gtfonwLEMRv9U1YHR9W58GfpjKsnL5jruTytr1JI8MG33UBDl0H5VOb+/DAx9SSOR5C3AV6vqx0neD5wBXF5V3xlx18aac/qSRuV9LfB/F/gD4JPAJ0bbpfFn6Esalafa/QXAP1TVdvyJ5WVn6Esalf9O8o/AW4Cb2hcJzaRl5py+pJFI8hvAegZ/I3d3kpOB11TV10bctbFm6EtSR/xfKUnqiKEvSR0x9CWpI4a+JHXE0Jekjvwfzqgf1ODbZkMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "result=Email_Data['Target'].value_counts()\n",
    "result.plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# بخش بندی دیتاست\n",
    "train, test = train_test_split(Email_Data[['Email', 'Target']] , test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# تعریف سری طول ها، ماکزیمم تعداد واژگان و ابعاد گنجاند یا امبدینگ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20000 واژگان برتر\n",
    "MAX_NB_WORDS = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#نحوه شناسایی واژگان معمولی  که موارد استفاده قرار میگیرند\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(train.Email)\n",
    "train_sequences = tokenizer.texts_to_sequences(train.Email)\n",
    "test_sequences = tokenizer.texts_to_sequences(test.Email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7576 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# dictionary containing words and their index\n",
    "word_index = tokenizer.word_index\n",
    "# print(tokenizer.word_index)\n",
    "# total words in the corpus\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# get only the top frequent words on train\n",
    "train_data = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get only the top frequent words on test\n",
    "test_data = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4457, 300)\n",
      "(1115, 300)\n"
     ]
    }
   ],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train['Target']\n",
    "test_labels = test['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(train_labels)\n",
    "train_labels = le.transform(train_labels)\n",
    "test_labels = le.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ham' 'spam']\n",
      "(array([0, 1]), array([3859,  598], dtype=int64))\n",
      "(array([0, 1]), array([966, 149], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "print(le.classes_)\n",
    "print(np.unique(train_labels, return_counts=True))\n",
    "print(np.unique(test_labels, return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (4457, 300)\n",
      "Shape of label tensor: (4457, 2)\n",
      "Shape of label tensor: (1115, 2)\n"
     ]
    }
   ],
   "source": [
    "from keras import utils as np_utils\n",
    "from keras.utils import to_categorical\n",
    "# changing data types\n",
    "labels_train = to_categorical(np.asarray(train_labels))\n",
    "labels_test = to_categorical(np.asarray(test_labels))\n",
    "print('Shape of data tensor:', train_data.shape)\n",
    "print('Shape of label tensor:', labels_train.shape)\n",
    "print('Shape of label tensor:', labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "print(MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ساخت مدل شبکه عصبی CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.layers import Dense, Input, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CNN 1D model.\n"
     ]
    }
   ],
   "source": [
    "print('Training CNN 1D model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv1D(128, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(5))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ShahinN\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 4457 samples, validate on 1115 samples\n",
      "Epoch 1/5\n",
      "4457/4457 [==============================] - 86s 19ms/step - loss: 0.3881 - accuracy: 0.8470 - val_loss: 0.4676 - val_accuracy: 0.8664\n",
      "Epoch 2/5\n",
      "4457/4457 [==============================] - 91s 20ms/step - loss: 0.1372 - accuracy: 0.9594 - val_loss: 0.9748 - val_accuracy: 0.8664\n",
      "Epoch 3/5\n",
      "4457/4457 [==============================] - 75s 17ms/step - loss: 0.0568 - accuracy: 0.9852 - val_loss: 1.0995 - val_accuracy: 0.8664\n",
      "Epoch 4/5\n",
      "4457/4457 [==============================] - 75s 17ms/step - loss: 0.0366 - accuracy: 0.9919 - val_loss: 0.9774 - val_accuracy: 0.8664\n",
      "Epoch 5/5\n",
      "4457/4457 [==============================] - 74s 17ms/step - loss: 0.0198 - accuracy: 0.9960 - val_loss: 0.8521 - val_accuracy: 0.8664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x11bb71079b0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, labels_train ,validation_data=(test_data, labels_test), batch_size=64, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SIMPLERNN model.\n"
     ]
    }
   ],
   "source": [
    "#import library\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "\n",
    "#model training\n",
    "print('Training SIMPLERNN model.')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "model.add(SimpleRNN(2, input_shape=(None,1)))\n",
    "model.add(Dense(2,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4457 samples, validate on 1115 samples\n",
      "Epoch 1/5\n",
      "4457/4457 [==============================] - 54s 12ms/step - loss: 0.3455 - accuracy: 0.9446 - val_loss: 0.2078 - val_accuracy: 0.9848\n",
      "Epoch 2/5\n",
      "4457/4457 [==============================] - 53s 12ms/step - loss: 0.1159 - accuracy: 0.9915 - val_loss: 0.1419 - val_accuracy: 0.9740\n",
      "Epoch 3/5\n",
      "4457/4457 [==============================] - 62s 14ms/step - loss: 0.0513 - accuracy: 0.9969 - val_loss: 0.1246 - val_accuracy: 0.9740\n",
      "Epoch 4/5\n",
      "4457/4457 [==============================] - 55s 12ms/step - loss: 0.0271 - accuracy: 0.9991 - val_loss: 0.1216 - val_accuracy: 0.9668\n",
      "Epoch 5/5\n",
      "4457/4457 [==============================] - 56s 12ms/step - loss: 0.0165 - accuracy: 0.9996 - val_loss: 0.1190 - val_accuracy: 0.9668\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x11bced75240>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, labels_train, batch_size=16, epochs=5, validation_data=(test_data, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.68528855, 0.31471145],\n",
       "       [0.9151998 , 0.08480018],\n",
       "       [0.995443  , 0.004557  ],\n",
       "       ...,\n",
       "       [0.9783735 , 0.02162646],\n",
       "       [0.9677645 , 0.03223554],\n",
       "       [0.84436935, 0.15563057]], dtype=float32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prediction on test data\n",
    "predicted_Srnn=model.predict(test_data)\n",
    "predicted_Srnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model evaluation\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "precision, recall, fscore, support = score(labels_test, predicted_Srnn.round())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.96777442 0.95901639]\n",
      "recall: [0.99482402 0.7852349 ]\n",
      "fscore: [0.98111281 0.86346863]\n",
      "support: [966 149]\n",
      "############################\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       966\n",
      "           1       0.96      0.79      0.86       149\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      1115\n",
      "   macro avg       0.96      0.89      0.92      1115\n",
      "weighted avg       0.97      0.97      0.97      1115\n",
      " samples avg       0.97      0.97      0.97      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "print(\"############################\")\n",
    "print(sklearn.metrics.classification_report(labels_test, predicted_Srnn.round()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ساخت مدل LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LSTM model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ShahinN\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(activation=\"relu\", return_sequences=True, units=16, recurrent_activation=\"hard_sigmoid\")`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#model training\n",
    "print('Training LSTM model.')\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH))\n",
    "model.add(LSTM(output_dim=16, activation='relu', inner_activation='hard_sigmoid',return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(2,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'binary_crossentropy', optimizer='adam',metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4457 samples, validate on 1115 samples\n",
      "Epoch 1/5\n",
      "4457/4457 [==============================] - 299s 67ms/step - loss: 0.1342 - accuracy: 0.9545 - val_loss: 0.2203 - val_accuracy: 0.9184\n",
      "Epoch 2/5\n",
      "4457/4457 [==============================] - 293s 66ms/step - loss: 0.0184 - accuracy: 0.9951 - val_loss: 0.0745 - val_accuracy: 0.9812\n",
      "Epoch 3/5\n",
      "4457/4457 [==============================] - 401s 90ms/step - loss: 0.0041 - accuracy: 0.9993 - val_loss: 0.0766 - val_accuracy: 0.9803\n",
      "Epoch 4/5\n",
      "4457/4457 [==============================] - 406s 91ms/step - loss: 0.0012 - accuracy: 0.9998 - val_loss: 0.1059 - val_accuracy: 0.9812\n",
      "Epoch 5/5\n",
      "4457/4457 [==============================] - 415s 93ms/step - loss: 0.0011 - accuracy: 0.9998 - val_loss: 0.1062 - val_accuracy: 0.9821\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x11bffedbf28>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, labels_train, batch_size=16, epochs=5, validation_data=(test_data, labels_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.4820095e-10, 1.0000000e+00],\n",
       "       [9.9995458e-01, 4.5383797e-05],\n",
       "       [1.0000000e+00, 2.9247932e-10],\n",
       "       ...,\n",
       "       [9.9998820e-01, 1.1766316e-05],\n",
       "       [9.9436921e-01, 5.6307805e-03],\n",
       "       [9.9959332e-01, 4.0665350e-04]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction on text data\n",
    "predicted_lstm=model.predict(test_data)\n",
    "predicted_lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: [0.97971602 1.        ]\n",
      "recall: [1.         0.86577181]\n",
      "fscore: [0.9897541  0.92805755]\n",
      "support: [966 149]\n",
      "############################\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       966\n",
      "           1       1.00      0.87      0.93       149\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      1115\n",
      "   macro avg       0.99      0.93      0.96      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      " samples avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "precision, recall, fscore, support = score(labels_test, predicted_lstm.round())\n",
    "\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "print('support: {}'.format(support))\n",
    "print(\"############################\")\n",
    "print(sklearn.metrics.classification_report(labels_test,predicted_lstm.round()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
